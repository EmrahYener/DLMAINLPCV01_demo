{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "Kopie von nlp_2-1_text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmrahYener/DLMAINLPCV01_demo/blob/master/Kopie_von_nlp_2_1_text_summarization_20_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGamZ8S_zyb"
      },
      "source": [
        "# **Text Summarization**\n",
        "\n",
        "Text summarization in NLP describes methods to automatically generate text summaries containing the most relevant information from source texts. With text summarization, we use extractive and abstractive techniques. In extractive techniques, algorithms extract the most important word sequences of the document to produce a summary of the given text. Abstractive techniques generate summaries by generating a new text and paraphrase the content of the original document, pretty much like humans do when they write an abstract [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "This notebook shows an example of unsupervised extractive text summerization with TextRank."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised extractive text summarization with TextRank\n",
        "\n",
        "TextRank is a common unsupervised extractive summarization technique. It compares every sentence in the text with every other sentence by calculating a similarity score, for example, the cosine similarity for each sentence pair. The closer the score is to 1, the more similar the sentence is to the other sentence representing the other sentences in a good way. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. Finally, the sentences can be sorted by rank and a summary can be built from a defined number of highest ranked sentences [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "Unsupervised text summarization can be performed with the ``spaCy`` library and the TextRank algorithm by using the ``pytextrank`` library. For more details about ``spaCy`` and ``pytextrank`` libraries, please refer to [[2]](https://spacy.io/) and [[3]](https://derwen.ai/docs/ptr/).\n",
        "\n",
        "The following example is based on [[4]](https://derwen.ai/docs/ptr/explain_summ/).\n",
        "\n",
        "\n",
        "**EDIT:**\n",
        "For tetxt summarization we will apply the following steps:\n",
        "* Install libraries\n",
        "* Download and install language model\n",
        "* Create a document\n",
        "* Perform sentence tokenization\n",
        "* Score each sentence\n",
        "* Rank each sentence by those scores\n",
        "* The top scoring sentences will be our summary"
      ],
      "metadata": {
        "id": "3HEtUqo6Z2lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "yUmJvv_rJ0sN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavxSpS-f3CI"
      },
      "source": [
        "#### Install ``pytextrank`` library\n",
        "\n",
        "``pytextrank`` is an implementation of TextRank to use in ``spaCy`` pipelines. It provides fast, effective phrase extraction from texts, along with extractive summarization [[5]](https://spacy.io/universe/project/spacy-pytextrank).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIo1p4uAC8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db6b8ff-74a5-438f-d3ed-1803e481c1a4"
      },
      "source": [
        "# Install the pytextrank library \n",
        "!pip install pytextrank==3.0.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank==3.0.1\n",
            "  Downloading pytextrank-3.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (3.3.1)\n",
            "Collecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (2.6.3)\n",
            "Collecting icecream>=2.1\n",
            "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank==3.0.1) (2.6.1)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank==3.0.1) (1.15.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.7.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.64.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (8.0.17)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank==3.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank==3.0.1) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank==3.0.1) (2.0.1)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed asttokens-2.0.5 colorama-0.4.5 executing-0.8.3 graphviz-0.20 icecream-2.1.2 pytextrank-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsF9N9igEwQ"
      },
      "source": [
        "#### Import libraries\n",
        "\n",
        "We import ``spaCy`` and ``pytextrank`` libraries.\n",
        "\n",
        "``spaCy`` is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning [[6]](https://spacy.io/usage/spacy-101). For example, it supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=8Pzkt1Z_M6OH). More information about ``spaCy`` please refer to  [[2]](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3QK56i_zyx"
      },
      "source": [
        "# Import spaCy and pytextrank libraries\n",
        "import spacy\n",
        "import pytextrank"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and install language model\n",
        "We load the ``en_core_web_sm`` English language model by using the ``spaCy`` library.\n",
        "For more details about ``en_core_web_sm``, please refer to [[7]](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "ibonn_5oG5BP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdAZJnug3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27cfc6f-52c9-4610-f8a0-f6a923e214ca"
      },
      "source": [
        "# Download \"en_core_web_sm\" English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load installed language model\n",
        "We use the ``spacy.load()`` function to load our language model ``en_core_web_sm`` to the ``spaCy`` pipeline ``sp``.\n"
      ],
      "metadata": {
        "id": "WXS91gcsCCKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the language model with the package name\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fL34gYV6CbWn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare pipeline\n",
        "\n",
        "We use the ``add_pipe()`` method to add ``pytextrank`` to the ``spaCy`` pipeline ``sp``."
      ],
      "metadata": {
        "id": "fz8gz5rWIMDw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upP_NK5B_zy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5bc4bc7-184f-47b3-b074-78aad435ddba"
      },
      "source": [
        "# Add pytextrank to the spaCy pipeline\n",
        "sp.add_pipe('textrank', last=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRank at 0x7f39737bd190>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our ``spaCy`` pipeline is ready for text summarization. For this, we create a ``spaCy`` document in the following step."
      ],
      "metadata": {
        "id": "d2wjGl1dVWAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create ``spaCy`` document with sample text\n",
        "\n",
        "In this step, we add a sample text to the ``spaCy`` pipeline and creade a ``Doc`` object as ``doc``.\n",
        "\n",
        "When we create a ``Doc`` object by using the ``spaCy`` library, it automatically performs tokenization, NER and POS tagging processes for an input text. The following figure demonstrates the processing pipeline of a given text to create a ``Doc`` object [[5]](https://spacy.io/usage/processing-pipelines).\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "vABivFctEqvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text for summarization\n",
        "text=\"\"\"Alan Mathison Turing, a British mathematician and computer scientist,\\\n",
        " was one of the early pioneers of artificial intelligence. Turing (1950) describes \\\n",
        " the foundation of what was later called the Turing test. The experimental setup of \\\n",
        " the Turing test is as follows. A human interrogator uses a chat program to talk to \\\n",
        " two conversation partners: a chatbot and another human being. Both of them try to \\\n",
        " convince the interrogator that they are the human. If the interrogator is not able to \\\n",
        " identify the human through intense questioning, the machine is considered to have passed \\\n",
        " the Turing test. According to Turing, passing the test can lead to the conclusion that \\\n",
        " the machine’s intellectual power is on a level comparable to the human brain. While the \\\n",
        " Turing test has often been criticized because of its focus on functionality, the question \\\n",
        " of whether the machine is conscious about its answers remains open. Several attempts have \\\n",
        " been made to pass the Turing test, but it still remains an unresolved challenge.\"\"\"\n",
        "\n",
        "# Create a spaCy Doc object \"doc\" with the sample text\n",
        "doc = sp(text)"
      ],
      "metadata": {
        "id": "EQS_AVHfeLvB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List top-ranked phrases\n",
        "\n",
        "Above we have added ``pytextrank`` to the ``spaCy`` pipeline and created a Doc object ``doc`` with a sample text.\n",
        "\n",
        "Now we can access the ``pytextrank`` component within the ``spaCy`` pipeline, and use it to get more information about the document.\n",
        "\n",
        "We use the ``_.phrases`` attribute of ``pytextrank`` to print a list of top-ranked phrases in the document. The list contains:\n",
        "* ``phrase.rank``: Cosine similarity score of each phrase\n",
        "* ``phrase.count``: Count of related phrase in the text\n",
        "* ``phrase.text``: The phrase itself as string\n",
        "\n",
        "The closer the similarity score of a phrase is to 1, the more important it is for text summarization. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. "
      ],
      "metadata": {
        "id": "VBOQF3K5Wm7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top-ranked phrases\n",
        "for phrase in doc._.phrases:\n",
        "  if phrase.rank>0:\n",
        "    print(f'{phrase.rank:{20}} {phrase.count:{5}} {phrase.text:{5}}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI93wih7gxmg",
        "outputId": "5aff8fc5-ad34-4f1e-ead4-fd251cb4d683"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0.10726830758300748     1 artificial intelligence\n",
            " 0.09432284433635442     3 Turing\n",
            " 0.08117326093202007     1 intense questioning\n",
            " 0.07092000319088093     4 the Turing test\n",
            " 0.06616451656224065     1 another human being\n",
            " 0.06567762671637971     1 the human brain\n",
            " 0.06239413059510708     1 functionality\n",
            " 0.06110163052707078     2 Alan Mathison Turing\n",
            "0.060920568103383546     1 A human interrogator\n",
            " 0.05768369492512729     1 an unresolved challenge\n",
            " 0.05229790122459473     1 the machine’s intellectual power\n",
            "0.051471946777544623     1 the  Turing test\n",
            "  0.0495936352774933     1 two conversation partners\n",
            "0.048966965109190665     1 Several attempts\n",
            " 0.04892833059988179     1 the test\n",
            "0.047631209375115745     1 British\n",
            " 0.04757661717719924     2 the machine\n",
            " 0.04594356957071834     1 a chat program\n",
            " 0.04380664297520442     1 the early pioneers\n",
            "0.040621810082179084     1 a British mathematician and computer scientist\n",
            "0.033507600154270484     1 a chatbot\n",
            "0.031332584043474714     1 a level\n",
            "0.031273740225418165     1 its answers\n",
            " 0.02997381818861146     2 the human\n",
            "0.029365636886376573     1 the question\n",
            " 0.02922437140951194     2 the interrogator\n",
            " 0.02815976249651412     1 the conclusion\n",
            "0.027273614400138448     1 its focus\n",
            "0.025968654812125343     1 The experimental setup\n",
            " 0.02197042509559323     1 the foundation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform text summarization\n",
        "\n",
        "We use the ``summary`` method of ``pytextrank`` to run an extractive summarization. We set the following parameters:\n",
        "\n",
        "* ``limit_phrases``: It defines the maximum number of top-ranked phrases to use in the distance vectors. In this example, we set ``limit_phrases=3``.\n",
        "\n",
        "* ``limit_sentences``: It defines the total number of sentences to return for the extractive summarization. In this example, we set ``limit_sentences=3``.\n",
        "\n",
        "* ``preserve_order``: It preserves the order of sentences as they originally occurred in the source text. In this example, we set ``preserve_order=True``."
      ],
      "metadata": {
        "id": "-VpDJDSne3Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform text summarization\n",
        "summary = list(doc._.textrank.summary(limit_phrases=3, limit_sentences=3, preserve_order=True))\n",
        "for sent in summary:\n",
        "  print(sent,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVOdowvCZrrl",
        "outputId": "36b563a7-3b70-49fc-d22a-7054d9547dd1"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alan Mathison Turing, a British mathematician and computer scientist, was one of the early pioneers of artificial intelligence. \n",
            "\n",
            "Turing (1950) describes  the foundation of what was later called the Turing test. \n",
            "\n",
            "According to Turing, passing the test can lead to the conclusion that  the machine’s intellectual power is on a level comparable to the human brain. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://spacy.io/\n",
        "- [3] https://derwen.ai/docs/ptr/\n",
        "- [4] https://derwen.ai/docs/ptr/explain_summ/\n",
        "- [5] https://spacy.io/universe/project/spacy-pytextrank\n",
        "- [6] https://spacy.io/models\n",
        "- [7] https://spacy.io/usage/spacy-101\n",
        "- [8] https://spacy.io/usage/linguistic-features\n",
        "- [9] https://derwen.ai/docs/ptr/glossary/#lemma-graph\n",
        "- [10] https://aclanthology.org/Q15-1016/\n"
      ],
      "metadata": {
        "id": "8Pzkt1Z_M6OH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oMgGVZAivX"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}