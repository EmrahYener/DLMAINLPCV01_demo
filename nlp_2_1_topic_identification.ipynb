{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**topic identification**\n",
    "\n",
    "\n",
    "find in this notebook examples to following metacharacter setting can be found:\n",
    "- [Anchors](#anchors)\n",
    "- [Quantifiers](#quantifiers)\n",
    "- [Disjunctions](#disjunctions)\n",
    "- [Character Classes](#character_classes)\n",
    "\n",
    "python documentation:\n",
    "- Regular Expression Syntax [library \"re\"](https://docs.python.org/3/library/re.html)\n",
    "- How-to use re [re-how-to](https://docs.python.org/3/howto/regex.html#regex-howto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dtaset-file\n",
    "#\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"nlp_2_1_topic_identification.ipynb\")\n",
    "#\n",
    "# data_file = os.path.join(os.path.dirname(notebook_path), \"NLP\\data\\subset_NewsCategory_v2.json\")\n",
    "data_file = os.path.join(os.path.dirname(notebook_path), \"NLP\\data\\\\News_Category_Dataset_v2.json\")\n",
    "\n",
    "#\n",
    "#data_file = \"D://Python_382//NLP//data//News_Category_Dataset_v2.json\"\n",
    "# df = pd.read_json(data_file, orient=\"records\", lines=True)\n",
    "#\n",
    "fJson = open(data_file)\n",
    "fJson.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read data from json\n",
    "df = pd.read_json(data_file, orient=\"records\", lines=True)\n",
    "#\n",
    "data = pd.DataFrame()\n",
    "data[\"text\"] = df.headline + df.short_description\n",
    "data[\"labels\"] = df.category\n",
    "#\n",
    "labels = list(data[\"labels\"].unique())\n",
    "#\n",
    "# Convert labels in numeric values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "data[\"labels\"] = le.transform(data[\"labels\"])\n",
    "#\n",
    "train_df, eval_df = train_test_split(data, test_size=0.2)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create a classification model\n",
    "# model = ClassificationModel('bert', 'bert-base-uncased', num_labels=len(labels), use_cuda=False)\n",
    "model = ClassificationModel('bert', 'bert-base-uncased', use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ac4ddb68414c9eacdf2b5e92dcdc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160682.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "# model.train_model(train_df, output_dir = os.path.join(os.path.dirname(notebook_path), \"NLP/output/\"))\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "result, model_outputs, predictions = model.eval_model(eval_def) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72cf23ac4f64a67ab34dbb7d3b2f898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4380633a0f4820a562dcd5be8d417b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d232814c5814b08ac91e55da539aeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "#\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
