{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-5_word_sense_disambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgPmSJ95ix2q"
      },
      "source": [
        "# **Word Sense Disambiguation**\n",
        "\n",
        "Words can have different meanings in different contexts. Sometimes the intended\n",
        "meaning of a word is hard to understand and leads to miscommunication. If a word has multiple meanings, this is called word\n",
        "sense ambiguity. While solving syntactic ambiguity is done with part-of-speech (POS)\n",
        "tagging, solving semantic ambiguity is done with word sense disambiguation (WSD).\n",
        "The challenge is to semantically separate words by their meaning in context [[1]](#scrollTo=fPge5oRLQwid).\n",
        "\n",
        "This notebook shows some basic WSD examples with ``pywsd`` library."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **``pywsd``**\n",
        "``pywsd`` is a Python library that provides WSD functions as well as several variations of the Lesk algorithm [[1]](#scrollTo=fPge5oRLQwid).\n",
        "\n",
        "For more detail about ``pywsd``, please refer to [[4]](https://pypi.org/project/pywsd/)."
      ],
      "metadata": {
        "id": "XnvDSfYy-YWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "45TfUvkfi9QU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Pywsd"
      ],
      "metadata": {
        "id": "_csm4xc0R-_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pywsd  "
      ],
      "metadata": {
        "id": "4_feI89HA9Pg",
        "outputId": "83e8719b-29cd-40fa-e75f-59a7841472d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pywsd in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.21.6)\n",
            "Requirement already satisfied: wn in /usr/local/lib/python3.7/dist-packages (from pywsd) (0.0.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pywsd) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install ``wn``\n",
        "``wn`` is a new Python library for working with wordnets. Unlike previous libraries, ``wn`` is built from the beginning to accommodate multiple wordnets (for multiple languages or multiple versions of the same wordnet) while retaining the ability to query and traverse them independently. For more detail about the ``wn`` library, please refer to [[5]](https://pypi.org/project/wn/) and [[6]](https://aclanthology.org/2021.gwc-1.12/).\n",
        "\n"
      ],
      "metadata": {
        "id": "FpGN8eXeSFxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wn==0.0.22"
      ],
      "metadata": {
        "id": "YmpRgxijC8oV",
        "outputId": "1f155341-e051-411f-d550-367bcf558eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wn==0.0.22 in /usr/local/lib/python3.7/dist-packages (0.0.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import ``nltk`` and ``wordnet``\n",
        "\n",
        "``nltk``(Natural Language Toolkit) is an open source Python library for natural language processing. For more detail about ``nltk``, please refer to [[2]](https://www.nltk.org/api/nltk.html#nltk.wsd.lesk).\n",
        "\n",
        "``wordnet`` is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations [[3]](http://www.nltk.org/howto/wsd.html). \n"
      ],
      "metadata": {
        "id": "GdGgQWKKUQok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk module\n",
        "import nltk\n",
        "\n",
        "# Download \"wordnet\" package by using the nltk module\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# The module 'averaged_perceptron_tagger' is used for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# The module \"punkt\" is used for tokenizing sentences \n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "B4Hqv8tCMMf3",
        "outputId": "f4d501b7-9a98-4db4-b3b5-4f9d22eafffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import ``simple_lesk``\n",
        "\n",
        "``lesk`` algorithm is an example of a knowledge-based method and is based on contextual overlap of dictionary definitions. The approach is based on the assumption that words used together are also related to each other [[1]](#scrollTo=fPge5oRLQwid)."
      ],
      "metadata": {
        "id": "FHkuushxUcNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple_lesk returns the sense most suited to the given word as per the Simple LESK Algorithm\n",
        "from pywsd.lesk import simple_lesk  "
      ],
      "metadata": {
        "id": "n2hi79CYBEi1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WSD appication examples"
      ],
      "metadata": {
        "id": "7HwviBLLjCIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bank"
      ],
      "metadata": {
        "id": "G2gPMfkNWI8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample text which contains two sentences\n",
        "text1 = ['I went to the bank to deposit my money', 'The river bank was full of dead fishes']\n",
        "\n",
        "# Anaylze the first sentence and print the definition of the word \"bank\"\n",
        "print( \"=============== analyse sentence 1 =================\\n\")\n",
        "print (\"Context-1:\", text1[0])  \n",
        "answer1 = simple_lesk(text1[0],'bank')  \n",
        "print (\"Sense:\", answer1)  \n",
        "print (\"Definition : \", answer1.definition())  \n",
        "\n",
        "# Anaylze the second sentence and print the definition of the word \"bank\"\n",
        "print( \"\\n\\n=============== analyse sentence 2 =================\\n\")\n",
        "print (\"Context-2:\", text1[1])  \n",
        "answer2 = simple_lesk(text1[1],'bank')  \n",
        "print (\"Sense:\", answer2)  \n",
        "print (\"Definition : \", answer2.definition())  \n",
        "\n",
        "#for s in wn.synsets('fair'):\n",
        "#    print('\\t', s, s.definition())"
      ],
      "metadata": {
        "id": "IW471hbAMjJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de024ee1-6879-40ab-85a0-1847426a14de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== analyse sentence 1 =================\n",
            "\n",
            "Context-1: I went to the bank to deposit my money\n",
            "Sense: Synset('depository_financial_institution.n.01')\n",
            "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n",
            "\n",
            "\n",
            "=============== analyse sentence 2 =================\n",
            "\n",
            "Context-2: The river bank was full of dead fishes\n",
            "Sense: Synset('bank.n.01')\n",
            "Definition :  sloping land (especially the slope beside a body of water)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plant"
      ],
      "metadata": {
        "id": "6AD2Aw6uYB1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample text which contains two sentences\n",
        "text2 = ['The workers at the industrial plant were overworked.', 'The plant was no longer bearing flowers.']\n",
        "\n",
        "# Anaylze the first sentence and print the definition of the word \"plant\"\n",
        "print( \"=============== analyse sentence 1 =================\\n\")\n",
        "print (\"Context-1:\", text2[0])  \n",
        "answer1 = simple_lesk(text2[0],'plant')  \n",
        "print (\"Sense:\", answer1)  \n",
        "print (\"Definition : \", answer1.definition())  \n",
        "\n",
        "# Anaylze the second sentence and print the definition of the word \"plant\"\n",
        "print( \"\\n\\n=============== analyse sentence 2 =================\\n\")\n",
        "print (\"Context-2:\", text2[1])  \n",
        "answer2 = simple_lesk(text2[1],'plant')  \n",
        "print (\"Sense:\", answer2)  \n",
        "print (\"Definition : \", answer2.definition())  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrAM723hYImV",
        "outputId": "85baccb7-c8a7-4ad7-a3da-4f845687be10"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== analyse sentence 1 =================\n",
            "\n",
            "Context-1: The workers at the industrial plant were overworked.\n",
            "Sense: Synset('plant.n.01')\n",
            "Definition :  buildings for carrying on industrial labor\n",
            "\n",
            "\n",
            "=============== analyse sentence 2 =================\n",
            "\n",
            "Context-2: The plant was no longer bearing flowers.\n",
            "Sense: Synset('plant.v.01')\n",
            "Definition :  put or set (seeds, seedlings, or plants) into the ground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fair"
      ],
      "metadata": {
        "id": "d7T1C_OuYFKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample text which contains two sentences\n",
        "text3 = ['Everyone needs to be given a fair chance in the competition.', 'The annual fair in our city is next weekend.']\n",
        "\n",
        "# Anaylze the first sentence and print the definition of the word \"fair\"\n",
        "print( \"=============== analyse sentence 1 =================\\n\")\n",
        "print (\"Context-1:\", text3[0])  \n",
        "answer1 = simple_lesk(text3[0],'fair')  \n",
        "print (\"Sense:\", answer1)  \n",
        "print (\"Definition : \", answer1.definition())  \n",
        "\n",
        "# Anaylze the second sentence and print the definition of the word \"fair\"\n",
        "print( \"\\n\\n=============== analyse sentence 2 =================\\n\")\n",
        "print (\"Context-2:\", text3[1])  \n",
        "answer2 = simple_lesk(text3[1],'fair', 'n')  \n",
        "print (\"Sense:\", answer2)  \n",
        "print (\"Definition : \", answer2.definition())  "
      ],
      "metadata": {
        "id": "WhjZ7MPJYJOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a081cd-95f4-41fa-8581-3ca4e32604f4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== analyse sentence 1 =================\n",
            "\n",
            "Context-1: Everyone needs to be given a fair chance in the competition.\n",
            "Sense: Synset('honest.s.07')\n",
            "Definition :  gained or earned without cheating or stealing\n",
            "\n",
            "\n",
            "=============== analyse sentence 2 =================\n",
            "\n",
            "Context-2: The annual fair in our city is next weekend.\n",
            "Sense: Synset('fair.n.03')\n",
            "Definition :  a competitive exhibition of farm products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] NLP and Computer Vision_DLMAINLPCV01 Lecture Book\n",
        "- [2] https://www.nltk.org/api/nltk.html#nltk.wsd.lesk\n",
        "- [3] http://www.nltk.org/howto/wsd.html\n",
        "- [4] https://pypi.org/project/pywsd/\n",
        "- [5] https://pypi.org/project/wn/\n",
        "- [6] https://aclanthology.org/2021.gwc-1.12/\n"
      ],
      "metadata": {
        "id": "fPge5oRLQwid"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHARdTvlix3T"
      },
      "source": [
        "Copyright Â© 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}