{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_2-1_text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGamZ8S_zyb"
      },
      "source": [
        "# **Text-Summarization**\n",
        "\n",
        "Text summarization in NLP describes methods to automatically generate text summaries containing the most relevant information from source texts. With text summarization, we use extractive and abstractive techniques. In extractive techniques, algorithms extract the most important word sequences of the document to produce a summary of the given text. Abstractive techniques generate summaries by generating a new text and paraphrase the content of the original document, pretty much like humans do when they write an abstract. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised extractive text summarization with TextRank\n",
        "In this section, we focus on examples for unsupervised extractive text summerization with TextRank [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "TextRank is a common unsupervised extractive summarization technique. It compares every sentence in the text with every other sentence by calculating a similarity score, for example, the cosine similarity for each sentence pair. The closer the score is to 1, the more similar the sentence is to the other sentence representing the other sentences in a good way. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. Finally, the sentences can be sorted by rank and a summary can be built from a defined number of highest ranked sentences. TextRank is inspired by PageRank, an algorithm developed by Google that is used to rank web pages by their importance [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "Unsupervised text summarization can be tackled with spaCy and the TextRank algorithm by using the PyTextRank extension. For more detail abouot the spaCy and PyTextRank please refer to [[2]](https://spacy.io/) and [[3]](https://derwen.ai/docs/ptr/).\n",
        "\n",
        "The following example is based on code-snippet for pytextrank [[4]](https://derwen.ai/docs/ptr/explain_summ/)."
      ],
      "metadata": {
        "id": "3HEtUqo6Z2lt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavxSpS-f3CI"
      },
      "source": [
        "### Install PyTextRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIo1p4uAC8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d2e59c8-7c6b-49b5-fb97-4a12a6913c06"
      },
      "source": [
        "# Install PyTextRank \n",
        "## PyTextRank is a Python implementation of TextRank as a spaCy pipeline extension.\n",
        "!pip install pytextrank==3.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank==3.0.1\n",
            "  Downloading pytextrank-3.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (2.6.3)\n",
            "Collecting spacy>=3.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting icecream>=2.1\n",
            "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank==3.0.1) (2.6.1)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank==3.0.1) (1.15.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 48.0 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (21.3)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 41.6 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.21.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.7)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank==3.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank==3.0.1) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, executing, colorama, asttokens, spacy, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed asttokens-2.0.5 catalogue-2.0.7 colorama-0.4.4 executing-0.8.3 graphviz-0.20 icecream-2.1.2 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pytextrank-3.0.1 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load language model\n",
        "We will download \"en_core_web_sm\" English language model by using spaCy library.\n",
        "It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities [[5]](https://spacy.io/models).\n",
        "It is optimized for CPU and its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer [[6]](https://spacy.io/models/en)."
      ],
      "metadata": {
        "id": "ibonn_5oG5BP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdAZJnug3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2e28ca-966d-4082-bd63-fd68685497b8"
      },
      "source": [
        "# Download \"en_core_web_sm\" English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.16)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsF9N9igEwQ"
      },
      "source": [
        "### Import libraries and warnings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcW1TC8xrynl"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3QK56i_zyx"
      },
      "source": [
        "# Import spaCy and pytextrank libraries\n",
        "import spacy\n",
        "import pytextrank\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare pipeline"
      ],
      "metadata": {
        "id": "fz8gz5rWIMDw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upP_NK5B_zy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a632f0a4-ab92-4567-9cf9-1829d4bf7838"
      },
      "source": [
        "# prepare pipeline\n",
        "sp.add_pipe('textrank', last=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRank at 0x7f71d8f9f550>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijhBuc6be9bz"
      },
      "source": [
        "#### Basic Text-Summarization example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKOHaC9Me80Z"
      },
      "source": [
        "# Create sample text as SpaCy instance\n",
        "doc = sp(\n",
        "    \"Mr. and Mrs. Dursley, of number four, Private Drive, were proud to say \\\n",
        "they were perfectly normal, thank you very much. They were the last people you'd expect \\\n",
        "to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\"\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOffQTR7od4X",
        "outputId": "6856bbfd-1856-4540-d2ba-0dffc2759511"
      },
      "source": [
        "# Print the noun chunks of the sample text\n",
        "for chunks in doc.noun_chunks:\n",
        "  print(chunks)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. and Mrs. Dursley\n",
            "number\n",
            "Private Drive\n",
            "they\n",
            "you\n",
            "They\n",
            "the last people\n",
            "you\n",
            "anything\n",
            "they\n",
            "such nonsense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSRfycdlpBjg",
        "outputId": "0df3b757-47a7-4665-f824-60747a7a9f11"
      },
      "source": [
        "# Print entities of the document\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, ent.start, ent.end)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dursley PERSON 3 4\n",
            "number four CARDINAL 6 8\n",
            "Private Drive FAC 9 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwC3SusLqI3Y",
        "outputId": "54bf6353-ff2d-47bd-d2a5-de8e7d5b110e"
      },
      "source": [
        "# Show top rated phrases\n",
        "\n",
        "# Iterate through each sentence in the doc, constructing a\n",
        "# [*lemma graph*](https://derwen.ai/docs/ptr/glossary/#lemma-graph),\n",
        "# then returning the top-ranked phrases.\n",
        "\n",
        "for p in doc._.phrases:\n",
        "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
        "    print(p.chunks)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2058     1  such nonsense\n",
            "[such nonsense]\n",
            "0.1477     2  Private Drive\n",
            "[Private Drive, Private Drive]\n",
            "0.1000     1  number\n",
            "[number]\n",
            "0.0883     1  Dursley\n",
            "[Dursley]\n",
            "0.0697     1  Mr. and Mrs. Dursley\n",
            "[Mr. and Mrs. Dursley]\n",
            "0.0520     1  the last people\n",
            "[the last people]\n",
            "0.0462     1  number four\n",
            "[number four]\n",
            "0.0000     1  They\n",
            "[They]\n",
            "0.0000     1  anything\n",
            "[anything]\n",
            "0.0000     2  they\n",
            "[they, they]\n",
            "0.0000     2  you\n",
            "[you, you]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVUNK5qQqORa",
        "outputId": "607266c0-bd28-4fa7-cc91-b714b493a99b"
      },
      "source": [
        "# Construct a list of the sentence boundaries with a phrase-vector (initialized to empty set) for each sentence.\n",
        "sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
        "print(sent_bounds)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, set()], [26, 53, set()]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1EI-aHeqUNu",
        "outputId": "be1fb9cb-e7f1-495d-a5f2-cfa23ca182fd"
      },
      "source": [
        "# Iterate through the top-ranked phrases and add them to the \n",
        "# phrase-vector for each sentence.\n",
        "\n",
        "limit_phrases = 4\n",
        "\n",
        "phrase_id = 0\n",
        "unit_vector = []\n",
        "\n",
        "for p in doc._.phrases:\n",
        "    print(phrase_id, p.text, p.rank)\n",
        "\n",
        "    unit_vector.append(p.rank)\n",
        "\n",
        "    for chunk in p.chunks:\n",
        "        #print(\" \", chunk.start, chunk.end)\n",
        "\n",
        "        for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "            if chunk.start >= sent_start and chunk.end <= sent_end:\n",
        "                #print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n",
        "                sent_vector.add(phrase_id)\n",
        "                break\n",
        "\n",
        "    phrase_id += 1\n",
        "\n",
        "    if phrase_id == limit_phrases:\n",
        "        break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 such nonsense 0.20578727368601005\n",
            "1 Private Drive 0.1476932514643156\n",
            "2 number 0.10003737208485038\n",
            "3 Dursley 0.08830619471948406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-2d6_Yuqitq",
        "outputId": "c48bbb1c-09bc-4bd9-b313-aaac106da29c"
      },
      "source": [
        "# Show the results\n",
        "\n",
        "# Look at the sentence boundaries with its phrase-vector\n",
        "print(sent_bounds)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, {1, 2, 3}], [26, 53, {0}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfN0utbbq5By",
        "outputId": "601a3306-4880-4b76-c22c-85d7eaf2ecae"
      },
      "source": [
        "# We also construct a unit_vector for all of the phrases, up to the limit requested.\n",
        "print(unit_vector)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20578727368601005, 0.1476932514643156, 0.10003737208485038, 0.08830619471948406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZJJnUBq6XH",
        "outputId": "1e22b390-1085-4997-9537-f1372ec9fad9"
      },
      "source": [
        "# Nomralize the unit_vector\n",
        "sum_ranks = sum(unit_vector)\n",
        "unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
        "\n",
        "unit_vector"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3798045837046876,\n",
              " 0.2725852424381945,\n",
              " 0.18463071976729584,\n",
              " 0.1629794540898221]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-a65t24q_cP",
        "outputId": "4e552a74-557c-4762-e984-8bea091f7d2e"
      },
      "source": [
        "# Iterate through each sentence, calculating its euclidean distance from the unit vector.\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "sent_rank = {}\n",
        "sent_id = 0\n",
        "\n",
        "for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "    #print(sent_vector)\n",
        "    sum_sq = 0.0\n",
        "\n",
        "    for phrase_id in range(len(unit_vector)):\n",
        "        #print(phrase_id, unit_vector[phrase_id])\n",
        "\n",
        "        if phrase_id not in sent_vector:\n",
        "            sum_sq += unit_vector[phrase_id]**2.0\n",
        "\n",
        "    sent_rank[sent_id] = sqrt(sum_sq)\n",
        "    sent_id += 1\n",
        "\n",
        "print(sent_rank)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.3798045837046876, 1: 0.3673602040672008}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du23UfWMrGE5",
        "outputId": "68aa9aa9-9584-42c3-d45e-fbce239f8c7d"
      },
      "source": [
        "# Sort the sentence indexes in descending order\n",
        "from operator import itemgetter\n",
        "\n",
        "sorted(sent_rank.items(), key=itemgetter(1)) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0.3673602040672008), (0, 0.3798045837046876)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZyjO0krJbk",
        "outputId": "b7a37045-03eb-41ea-fc13-f9ebdba99011"
      },
      "source": [
        "# Extract the sentences with the lowest distance, up to the limit requested.\n",
        "limit_sentences = 1\n",
        "\n",
        "sent_text = {}\n",
        "sent_id = 0\n",
        "\n",
        "for sent in doc.sents:\n",
        "    sent_text[sent_id] = sent.text\n",
        "    sent_id += 1\n",
        "\n",
        "num_sent = 0\n",
        "\n",
        "for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
        "    print(sent_id, sent_text[sent_id])\n",
        "    num_sent += 1\n",
        "\n",
        "    if num_sent == limit_sentences:\n",
        "        break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] NLP and Computer Vision_DLMAINLPCV01 Course Book\n",
        "- [2] https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
        "\n",
        "https://www.kaggle.com/code/aggarwalrahul/nlp-text-summarization-using-textrank\n",
        "\n",
        "https://colab.research.google.com/github/prateekjoshi565/textrank_text_summarization/blob/master/TestRank_Text_Summarization.ipynb#scrollTo=jwxtPBlgO_Gk\n",
        "\n",
        "dataset: https://www.kaggle.com/code/emrahyener/nlp-text-summarization-using-textrank/edit"
      ],
      "metadata": {
        "id": "8Pzkt1Z_M6OH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oMgGVZAivX"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}