{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_2-1_text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGamZ8S_zyb"
      },
      "source": [
        "# **Text Summarization**\n",
        "\n",
        "Text summarization in NLP describes methods to automatically generate text summaries containing the most relevant information from source texts. With text summarization, we use extractive and abstractive techniques. In extractive techniques, algorithms extract the most important word sequences of the document to produce a summary of the given text. Abstractive techniques generate summaries by generating a new text and paraphrase the content of the original document, pretty much like humans do when they write an abstract [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "This notebook shows an example of unsupervised extractive text summerization with TextRank."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised extractive text summarization with TextRank\n",
        "\n",
        "TextRank is a common unsupervised extractive summarization technique. It compares every sentence in the text with every other sentence by calculating a similarity score, for example, the cosine similarity for each sentence pair. The closer the score is to 1, the more similar the sentence is to the other sentence representing the other sentences in a good way. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. Finally, the sentences can be sorted by rank and a summary can be built from a defined number of highest ranked sentences [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "Unsupervised text summarization can be performed with the ``spaCy`` library and the TextRank algorithm by using the ``pytextrank`` library. For more details about ``spaCy`` and ``pytextrank`` libraries, please refer to [[2]](https://spacy.io/) and [[3]](https://derwen.ai/docs/ptr/).\n",
        "\n",
        "The following example is based on [[4]](https://derwen.ai/docs/ptr/explain_summ/)."
      ],
      "metadata": {
        "id": "3HEtUqo6Z2lt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavxSpS-f3CI"
      },
      "source": [
        "### Install ``pytextrank`` library\n",
        "\n",
        "``pytextrank`` is an implementation of TextRank to use in ``spaCy`` pipelines. It provides fast, effective phrase extraction from texts, along with extractive summarization [[5]](https://spacy.io/universe/project/spacy-pytextrank).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIo1p4uAC8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb2c92d-a117-4927-d9ed-81e0bb6f806a"
      },
      "source": [
        "# Install the pytextrank library \n",
        "!pip install pytextrank==3.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank==3.0.1\n",
            "  Downloading pytextrank-3.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting icecream>=2.1\n",
            "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (2.6.3)\n",
            "Collecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (3.3.1)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank==3.0.1) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank==3.0.1) (1.15.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.7.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.4.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.64.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank==3.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank==3.0.1) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank==3.0.1) (2.0.1)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed asttokens-2.0.5 colorama-0.4.5 executing-0.8.3 graphviz-0.20 icecream-2.1.2 pytextrank-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and install language model\n",
        "We load the ``en_core_web_sm`` English language model by using the ``spaCy`` library.\n",
        "For more details about ``en_core_web_sm``, please refer to [[6]](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "ibonn_5oG5BP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdAZJnug3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a5a5dd-af97-44db-86bb-d7d8fa53a773"
      },
      "source": [
        "# Download \"en_core_web_sm\" English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsF9N9igEwQ"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "We import ``spaCy`` and ``pytextrank`` libraries.\n",
        "\n",
        "``spaCy`` is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning [[7]](https://spacy.io/usage/spacy-101). For example, it supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=8Pzkt1Z_M6OH). More information about ``spaCy`` please refer to  [[2]](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3QK56i_zyx"
      },
      "source": [
        "# Import spaCy and pytextrank libraries\n",
        "import spacy\n",
        "import pytextrank"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load installed language model\n"
      ],
      "metadata": {
        "id": "WXS91gcsCCKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the language model with the package name\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fL34gYV6CbWn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare pipeline\n",
        "\n",
        "We use the ``add_pipe()`` method to add a component to the processing pipeline. We will add ``pytextrank`` to the ``spaCy`` pipeline."
      ],
      "metadata": {
        "id": "fz8gz5rWIMDw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upP_NK5B_zy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc795a67-c570-44f4-f8e7-c7cc4009eb4f"
      },
      "source": [
        "# Add pytextrank to the spaCy pipeline\n",
        "sp.add_pipe('textrank', last=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRank at 0x7fd9acc47850>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijhBuc6be9bz"
      },
      "source": [
        "### Text summarization example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create sample text"
      ],
      "metadata": {
        "id": "vABivFctEqvw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKOHaC9Me80Z"
      },
      "source": [
        "# Create sample text as spaCy Doc object\n",
        "doc = sp(\n",
        "    \"\"\n",
        ")Mr. and Mrs. Dursley, of number four, Private Drive, were proud to say \\\n",
        "they were perfectly normal, thank you very much. They were the last people you'd expect \\\n",
        "to be involved in anything strange or mysterious, because they just didn't hold with such nonsense."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print noun chunks\n",
        "\n",
        "Usually, in TextRank distances of each sentence to each other sentence are computed. But comparing a lot of sentences with each other can come with high computational costs. Consequently, to save computational costs, we show an approach where 4 representative noun chunks are combined in a so called unit vector which represents the whole sample text. This unit vector is then compared to each sentence. This way it is not necessary to compare each sentence with each other.\n",
        "\n",
        "We use ``noun_chunks`` method to iterate over the base noun phrases in the document. "
      ],
      "metadata": {
        "id": "nnVb9g1YEwrH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOffQTR7od4X",
        "outputId": "be9c818e-3cde-4314-fc8a-a980572f83b0"
      },
      "source": [
        "# Print the noun chunks of the sample text\n",
        "for chunks in doc.noun_chunks:\n",
        "  print(chunks)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. and Mrs. Dursley\n",
            "number\n",
            "Private Drive\n",
            "they\n",
            "you\n",
            "They\n",
            "the last people\n",
            "you\n",
            "anything\n",
            "they\n",
            "such nonsense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Show top rated noun chunks\n",
        "To extract the important 4 resepresentative noun chunks which will later be combined in the unit vector, we first rank all noun chunks. For that, we iterate through each sentence in the ``doc`` and rank our noun chunks with a ranking algorithm provided by TextRank. This algorithm is based on the creation of a lemma graph which returns the top-ranked phrases [[9]](https://derwen.ai/docs/ptr/glossary/#lemma-graph). "
      ],
      "metadata": {
        "id": "ZlDhQsSAF8OQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwC3SusLqI3Y",
        "outputId": "979dac0a-43bf-49dc-ae99-7de8287150ec"
      },
      "source": [
        "# Show top rated phrases\n",
        "for p in doc._.phrases:\n",
        "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
        "    print(p.chunks)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2058     1  such nonsense\n",
            "[such nonsense]\n",
            "0.1477     2  Private Drive\n",
            "[Private Drive, Private Drive]\n",
            "0.1000     1  number\n",
            "[number]\n",
            "0.0883     1  Dursley\n",
            "[Dursley]\n",
            "0.0697     1  Mr. and Mrs. Dursley\n",
            "[Mr. and Mrs. Dursley]\n",
            "0.0520     1  the last people\n",
            "[the last people]\n",
            "0.0462     1  number four\n",
            "[number four]\n",
            "0.0000     1  They\n",
            "[They]\n",
            "0.0000     1  anything\n",
            "[anything]\n",
            "0.0000     2  they\n",
            "[they, they]\n",
            "0.0000     2  you\n",
            "[you, you]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a list of the sentence boundaries\n",
        "Create a list of the sentence boundaries with a phrase vector (initialized to an empty set) for each sentence. WARUM?"
      ],
      "metadata": {
        "id": "ybfL81bLGUIp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVUNK5qQqORa",
        "outputId": "31325f8c-2876-4f4f-c6ab-b4c665d69eb9"
      },
      "source": [
        "# Create a list of the sentence boundaries\n",
        "sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
        "print(sent_bounds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, set()], [26, 53, set()]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add top-ranked phrases to the phrase-vector \n",
        "Iterate through the 4 top-ranked phrases and add them to the phrase vector for each sentence."
      ],
      "metadata": {
        "id": "o1HOWWjoGdbz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1EI-aHeqUNu",
        "outputId": "865b03f3-084e-487d-aee9-7761f139b9a1"
      },
      "source": [
        "# Set number of the phrases\n",
        "## This defines the item number of the top-ranked phrases list\n",
        "no_phrases = 4\n",
        "\n",
        "# Set \"phrase_id\" to zero and increase after each iteration\n",
        "phrase_id = 0\n",
        "\n",
        "# Create an empty list as \"unit_vector\" to keep the rank data of the phrases\n",
        "unit_vector = []\n",
        "\n",
        "# List the top 4 phrases and append into unit_vector\n",
        "for p in doc._.phrases:\n",
        "    print(phrase_id, p.text, p.rank)\n",
        "\n",
        "    unit_vector.append(p.rank)\n",
        "\n",
        "    for chunk in p.chunks:\n",
        "        #print(\" \", chunk.start, chunk.end)\n",
        "\n",
        "        for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "            if chunk.start >= sent_start and chunk.end <= sent_end:\n",
        "                #print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n",
        "                sent_vector.add(phrase_id)\n",
        "                break\n",
        "\n",
        "    phrase_id += 1\n",
        "\n",
        "    if phrase_id == no_phrases:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 such nonsense 0.20578727368601005\n",
            "1 Private Drive 0.1476932514643156\n",
            "2 number 0.10003737208485038\n",
            "3 Dursley 0.08830619471948406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-2d6_Yuqitq",
        "outputId": "a7035371-120e-489f-af1e-ceb025c076c7"
      },
      "source": [
        "# Look at the sentence boundaries with its phrase vector\n",
        "## The phrases with id number 1,2 and 3 belongs to the first sentence\n",
        "## The phrase with id number 0 belongs to the second sentence\n",
        "print(sent_bounds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, {1, 2, 3}], [26, 53, {0}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print the unit_vector"
      ],
      "metadata": {
        "id": "fEBUNuc_GpbC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfN0utbbq5By",
        "outputId": "89cce717-e0e9-485e-f913-a5c3ad9fe697"
      },
      "source": [
        "# Print the unit vector which contains the rank data of the top 4 phrases\n",
        "print(unit_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20578727368601005, 0.1476932514643156, 0.10003737208485038, 0.08830619471948406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalize the unit_vector\n",
        "\n",
        "Vectors are normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent [[10]](https://aclanthology.org/Q15-1016/).\n",
        "\n"
      ],
      "metadata": {
        "id": "NxFkzMcoHATf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZJJnUBq6XH",
        "outputId": "8aaa8a69-c4d5-4279-809d-3d7d2221bb11"
      },
      "source": [
        "# Normalize the unit_vector\n",
        "sum_ranks = sum(unit_vector)\n",
        "unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
        "\n",
        "unit_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3798045837046876,\n",
              " 0.2725852424381945,\n",
              " 0.18463071976729584,\n",
              " 0.1629794540898221]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate euclidean distance for each sentence\n",
        "\n",
        "We iterate through each sentence and calculate its euclidean distance from the unit vector."
      ],
      "metadata": {
        "id": "4zccnGldHEjZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-a65t24q_cP",
        "outputId": "e28df4b6-1f61-4276-e2a5-27df27e30046"
      },
      "source": [
        "# sqrt function is used to return the square root of x\n",
        "from math import sqrt\n",
        "\n",
        "# Create a dictionary to keep rank data of the sentences\n",
        "sent_rank = {}\n",
        "\n",
        "# Set \"sent_id\" to zero and increase after each iteration\n",
        "sent_id = 0\n",
        "\n",
        "# Calculate sentence rank data and append into the dictionary \"sent_rank\"\n",
        "for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "    #print(sent_vector)\n",
        "    sum_sq = 0.0\n",
        "\n",
        "    for phrase_id in range(len(unit_vector)):\n",
        "        #print(phrase_id, unit_vector[phrase_id])\n",
        "\n",
        "        if phrase_id not in sent_vector:\n",
        "            sum_sq += unit_vector[phrase_id]**2.0\n",
        "\n",
        "    sent_rank[sent_id] = sqrt(sum_sq)\n",
        "    sent_id += 1\n",
        "\n",
        "print(sent_rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.3798045837046876, 1: 0.3673602040672008}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract the sentences\n",
        "\n",
        "We extract the sentences with the lowest distance, up to the requested sentence limit."
      ],
      "metadata": {
        "id": "RWLo7DnVHWGY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZyjO0krJbk",
        "outputId": "fa683790-7712-48a0-d61b-1c22d13dc09a"
      },
      "source": [
        "# Set the requested sentence limit\n",
        "limit_sentences = 1\n",
        "\n",
        "sent_text = {}\n",
        "sent_id = 0\n",
        "\n",
        "for sent in doc.sents:\n",
        "    sent_text[sent_id] = sent.text\n",
        "    sent_id += 1\n",
        "\n",
        "num_sent = 0\n",
        "\n",
        "# Extract and print the sentence with lowest distance\n",
        "for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
        "    print(sent_id, sent_text[sent_id])\n",
        "    num_sent += 1\n",
        "\n",
        "    if num_sent == limit_sentences:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://spacy.io/\n",
        "- [3] https://derwen.ai/docs/ptr/\n",
        "- [4] https://derwen.ai/docs/ptr/explain_summ/\n",
        "- [5] https://spacy.io/universe/project/spacy-pytextrank\n",
        "- [6] https://spacy.io/models\n",
        "- [7] https://spacy.io/usage/spacy-101\n",
        "- [8] https://spacy.io/usage/linguistic-features\n",
        "- [9] https://derwen.ai/docs/ptr/glossary/#lemma-graph\n",
        "- [10] https://aclanthology.org/Q15-1016/\n"
      ],
      "metadata": {
        "id": "8Pzkt1Z_M6OH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oMgGVZAivX"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}