{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_2-1_text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGamZ8S_zyb"
      },
      "source": [
        "# **Text-Summarization**\n",
        "\n",
        "Text summarization in NLP describes methods to automatically generate text summaries containing the most relevant information from source texts. With text summarization, we use extractive and abstractive techniques. In extractive techniques, algorithms extract the most important word sequences of the document to produce a summary of the given text. Abstractive techniques generate summaries by generating a new text and paraphrase the content of the original document, pretty much like humans do when they write an abstract [[1]](#scrollTo=8Pzkt1Z_M6OH)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised extractive text summarization with TextRank\n",
        "In this section, we focus on examples for unsupervised extractive text summerization with TextRank.\n",
        "\n",
        "TextRank is a common unsupervised extractive summarization technique. It compares every sentence in the text with every other sentence by calculating a similarity score, for example, the cosine similarity for each sentence pair. The closer the score is to 1, the more similar the sentence is to the other sentence representing the other sentences in a good way. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. Finally, the sentences can be sorted by rank and a summary can be built from a defined number of highest ranked sentences. TextRank is inspired by PageRank, an algorithm developed by Google that is used to rank web pages by their importance [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "Unsupervised text summarization can be tackled with the ``spacy`` library and the TextRank algorithm by using the ``pytextrank`` library. For more details about ``spacy`` and ``pytextrank`` libraries, please refer to [[2]](https://spacy.io/) and [[3]](https://derwen.ai/docs/ptr/).\n",
        "\n",
        "The following example is based on [[4]](https://derwen.ai/docs/ptr/explain_summ/)."
      ],
      "metadata": {
        "id": "3HEtUqo6Z2lt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavxSpS-f3CI"
      },
      "source": [
        "### Install ``pytextrank``\n",
        "\n",
        "``pytextrank`` is an implementation of TextRank in Python for use in ``spacy`` pipelines which provides fast, effective phrase extraction from texts, along with extractive summarization. The graph algorithm works independently of a specific natural language and does not require domain knowledge  [[5]](https://spacy.io/universe/project/spacy-pytextrank).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIo1p4uAC8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca5e6f8a-09d4-4c28-f47d-08db4eca708a"
      },
      "source": [
        "# Install PyTextRank \n",
        "!pip install pytextrank==3.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank==3.0.1\n",
            "  Downloading pytextrank-3.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting icecream>=2.1\n",
            "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (2.6.3)\n",
            "Collecting spacy>=3.0\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 3.2 MB/s \n",
            "\u001b[?25hCollecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank==3.0.1) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank==3.0.1) (1.15.0)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (21.3)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.6)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (57.4.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.64.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 22.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.11.3)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.4.1)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank==3.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank==3.0.1) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, executing, colorama, asttokens, spacy, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed asttokens-2.0.5 catalogue-2.0.7 colorama-0.4.4 executing-0.8.3 graphviz-0.20 icecream-2.1.2 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 pytextrank-3.0.1 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and install the language model\n",
        "We will download and install ``en_core_web_sm`` English language model by using ``spacy`` library.\n",
        "It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities [[6]](https://spacy.io/models).\n",
        "It is optimized for CPU and its components are: ``tok2vec``, ``tagger``, ``parser``, ``senter``, ``ner``, ``attribute_ruler``, ``lemmatizer`` [[7]](https://spacy.io/models/en)."
      ],
      "metadata": {
        "id": "ibonn_5oG5BP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdAZJnug3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83e6c30-3c7f-495e-9329-18aac534493e"
      },
      "source": [
        "# Download \"en_core_web_sm\" English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 568 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.16)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsF9N9igEwQ"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "We import ``spacy`` and ``pytextrank`` libraries.\n",
        "\n",
        "``spacy`` is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning [[8]](https://spacy.io/usage/spacy-101). For example, it supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=8Pzkt1Z_M6OH). More information about ``spacy`` please refer to  [[2]](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3QK56i_zyx"
      },
      "source": [
        "# Import spaCy and pytextrank libraries\n",
        "import spacy\n",
        "import pytextrank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the installed language model\n"
      ],
      "metadata": {
        "id": "WXS91gcsCCKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the language model with the package name or a path to the data directory\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fL34gYV6CbWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare pipeline\n",
        "\n",
        "We use ``add_pipe()`` method to add a component to the processing pipeline. We will add ``pytextrank`` to the ``spacy`` pipeline."
      ],
      "metadata": {
        "id": "fz8gz5rWIMDw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upP_NK5B_zy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475521a7-1de6-417d-e179-9c7a405b443e"
      },
      "source": [
        "# Add PyTextRank to the spaCy pipeline\n",
        "sp.add_pipe('textrank', last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRank at 0x7f1782a759d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijhBuc6be9bz"
      },
      "source": [
        "### Basic text-summarization example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create sample text"
      ],
      "metadata": {
        "id": "vABivFctEqvw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKOHaC9Me80Z"
      },
      "source": [
        "# Create sample text as SpaCy instance\n",
        "doc = sp(\n",
        "    \"Mr. and Mrs. Dursley, of number four, Private Drive, were proud to say \\\n",
        "they were perfectly normal, thank you very much. They were the last people you'd expect \\\n",
        "to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print the noun chunks\n",
        "\n",
        "We use ``noun_chunks`` method to iterate over the base noun phrases in the document. "
      ],
      "metadata": {
        "id": "nnVb9g1YEwrH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOffQTR7od4X",
        "outputId": "e06583f8-2105-494f-e601-1fe7ee938d61"
      },
      "source": [
        "# Print the noun chunks of the sample text\n",
        "for chunks in doc.noun_chunks:\n",
        "  print(chunks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. and Mrs. Dursley\n",
            "number\n",
            "Private Drive\n",
            "they\n",
            "you\n",
            "They\n",
            "the last people\n",
            "you\n",
            "anything\n",
            "they\n",
            "such nonsense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print entities of the document\n",
        "\n",
        "Named entities are available as the ``ents`` property of a ``doc``.\n",
        "The standard way to access entity annotations is the ``doc.ents`` property. The entity type is accessible either as a hash value or as a string, using the attributes ``ent.label`` and ``ent.label_`` [[9]](https://spacy.io/usage/linguistic-features)."
      ],
      "metadata": {
        "id": "hjFaObRRFOcI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSRfycdlpBjg",
        "outputId": "b411d1f0-0b16-4fa1-ab5c-694b43a33e57"
      },
      "source": [
        "# Print entities of the document\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, ent.start, ent.end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dursley PERSON 3 4\n",
            "number four CARDINAL 6 8\n",
            "Private Drive FAC 9 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Show top rated phrases\n",
        "We iterate through each sentence in the ``doc``, constructing a lemma graph, then returning the top-ranked phrases [[10]](https://derwen.ai/docs/ptr/glossary/#lemma-graph)."
      ],
      "metadata": {
        "id": "ZlDhQsSAF8OQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwC3SusLqI3Y",
        "outputId": "70cb3416-8e93-4c23-e212-633c738d3b90"
      },
      "source": [
        "# Show top rated phrases\n",
        "for p in doc._.phrases:\n",
        "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
        "    print(p.chunks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2058     1  such nonsense\n",
            "[such nonsense]\n",
            "0.1477     2  Private Drive\n",
            "[Private Drive, Private Drive]\n",
            "0.1000     1  number\n",
            "[number]\n",
            "0.0883     1  Dursley\n",
            "[Dursley]\n",
            "0.0697     1  Mr. and Mrs. Dursley\n",
            "[Mr. and Mrs. Dursley]\n",
            "0.0520     1  the last people\n",
            "[the last people]\n",
            "0.0462     1  number four\n",
            "[number four]\n",
            "0.0000     1  They\n",
            "[They]\n",
            "0.0000     1  anything\n",
            "[anything]\n",
            "0.0000     2  they\n",
            "[they, they]\n",
            "0.0000     2  you\n",
            "[you, you]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a list of the sentence boundaries\n",
        "Create a list of the sentence boundaries with a phrase vector (initialized to an empty set) for each sentence."
      ],
      "metadata": {
        "id": "ybfL81bLGUIp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVUNK5qQqORa",
        "outputId": "ede8e2b8-523a-43f2-e57e-adef823aff1f"
      },
      "source": [
        "# Create a list of the sentence boundaries\n",
        "sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
        "print(sent_bounds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, set()], [26, 53, set()]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add top-ranked phrases to the phrase-vector \n",
        "Iterate through the top-ranked phrases and add them to the phrase vector for each sentence."
      ],
      "metadata": {
        "id": "o1HOWWjoGdbz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1EI-aHeqUNu",
        "outputId": "0c90d6d1-9323-4db6-fb8e-0e6d3a0c0808"
      },
      "source": [
        "# Type limit of the iteration\n",
        "## This defines the item number of the top-ranked phrases list\n",
        "limit_phrases = 4\n",
        "\n",
        "# Set \"phrase_id\" to zero and increase after each iteration\n",
        "phrase_id = 0\n",
        "\n",
        "# Create an empty list as \"unit_vector\" to keep the rank data of the phrases\n",
        "unit_vector = []\n",
        "\n",
        "# List the top 4 phrases and append into unit_vector\n",
        "for p in doc._.phrases:\n",
        "    print(phrase_id, p.text, p.rank)\n",
        "\n",
        "    unit_vector.append(p.rank)\n",
        "\n",
        "    for chunk in p.chunks:\n",
        "        #print(\" \", chunk.start, chunk.end)\n",
        "\n",
        "        for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "            if chunk.start >= sent_start and chunk.end <= sent_end:\n",
        "                #print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n",
        "                sent_vector.add(phrase_id)\n",
        "                break\n",
        "\n",
        "    phrase_id += 1\n",
        "\n",
        "    if phrase_id == limit_phrases:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 such nonsense 0.20578727368601005\n",
            "1 Private Drive 0.1476932514643156\n",
            "2 number 0.10003737208485038\n",
            "3 Dursley 0.08830619471948406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-2d6_Yuqitq",
        "outputId": "03f9e009-9cf5-4305-8d3e-2cbf59298c39"
      },
      "source": [
        "# Look at the sentence boundaries with its phrase-vector\n",
        "## The phrases with id number 1,2 and 3 belongs to the first sentence\n",
        "## The phrase with id number 0 belongs to the second sentence\n",
        "print(sent_bounds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 26, {1, 2, 3}], [26, 53, {0}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print the unit_vector"
      ],
      "metadata": {
        "id": "fEBUNuc_GpbC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfN0utbbq5By",
        "outputId": "16ae19be-8506-466b-bad6-47c459e18501"
      },
      "source": [
        "# Print the unit vector which contains the rank data of the top 4 phrases\n",
        "print(unit_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20578727368601005, 0.1476932514643156, 0.10003737208485038, 0.08830619471948406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalize the unit_vector\n",
        "\n",
        "Vectors are normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent [[11]](https://aclanthology.org/Q15-1016/).\n",
        "\n"
      ],
      "metadata": {
        "id": "NxFkzMcoHATf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZJJnUBq6XH",
        "outputId": "72fce2b6-29c6-4eca-834c-034079a1c1b9"
      },
      "source": [
        "# Normalize the unit_vector\n",
        "sum_ranks = sum(unit_vector)\n",
        "unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
        "\n",
        "unit_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3798045837046876,\n",
              " 0.2725852424381945,\n",
              " 0.18463071976729584,\n",
              " 0.1629794540898221]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate euclidean distance for each sentence"
      ],
      "metadata": {
        "id": "4zccnGldHEjZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-a65t24q_cP",
        "outputId": "a278d3c5-3399-46bf-bfe1-5750d2b21689"
      },
      "source": [
        "# Iterate through each sentence, calculating its euclidean distance from the unit vector\n",
        "\n",
        "## sqrt function is used to return the square root of x\n",
        "from math import sqrt\n",
        "\n",
        "## Create a dictionary to keep rank data of the sentences\n",
        "sent_rank = {}\n",
        "\n",
        "## Set \"sent_id\" to zero and increase after each iteration\n",
        "sent_id = 0\n",
        "\n",
        "## Calculate sentence rank data and append into the dictionary \"sent_rank\"\n",
        "for sent_start, sent_end, sent_vector in sent_bounds:\n",
        "    #print(sent_vector)\n",
        "    sum_sq = 0.0\n",
        "\n",
        "    for phrase_id in range(len(unit_vector)):\n",
        "        #print(phrase_id, unit_vector[phrase_id])\n",
        "\n",
        "        if phrase_id not in sent_vector:\n",
        "            sum_sq += unit_vector[phrase_id]**2.0\n",
        "\n",
        "    sent_rank[sent_id] = sqrt(sum_sq)\n",
        "    sent_id += 1\n",
        "\n",
        "print(sent_rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.3798045837046876, 1: 0.3673602040672008}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sort the sentence indexes"
      ],
      "metadata": {
        "id": "gfnfls9PHS2C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du23UfWMrGE5",
        "outputId": "f1d7f256-90f4-424d-a170-376aadaee3f1"
      },
      "source": [
        "# Sort the sentence indexes in descending order\n",
        "from operator import itemgetter\n",
        "\n",
        "sorted(sent_rank.items(), key=itemgetter(1)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0.3673602040672008), (0, 0.3798045837046876)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract the sentences"
      ],
      "metadata": {
        "id": "RWLo7DnVHWGY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZyjO0krJbk",
        "outputId": "c9623dcd-06ba-4491-9426-f64018c7010b"
      },
      "source": [
        "# Extract the sentences with the lowest distance, up to the limit requested.\n",
        "limit_sentences = 1\n",
        "\n",
        "sent_text = {}\n",
        "sent_id = 0\n",
        "\n",
        "for sent in doc.sents:\n",
        "    sent_text[sent_id] = sent.text\n",
        "    sent_id += 1\n",
        "\n",
        "num_sent = 0\n",
        "\n",
        "for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
        "    print(sent_id, sent_text[sent_id])\n",
        "    num_sent += 1\n",
        "\n",
        "    if num_sent == limit_sentences:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] NLP and Computer Vision_DLMAINLPCV01 Course Book\n",
        "- [2] https://spacy.io/\n",
        "- [3] https://derwen.ai/docs/ptr/\n",
        "- [4] https://derwen.ai/docs/ptr/explain_summ/\n",
        "- [5] https://spacy.io/universe/project/spacy-pytextrank\n",
        "- [6] https://spacy.io/models\n",
        "- [7] https://spacy.io/models/en\n",
        "- [8] https://spacy.io/usage/spacy-101\n",
        "- [9] https://spacy.io/usage/linguistic-features\n",
        "- [10] https://derwen.ai/docs/ptr/glossary/#lemma-graph\n",
        "- [11] https://aclanthology.org/Q15-1016/\n"
      ],
      "metadata": {
        "id": "8Pzkt1Z_M6OH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oMgGVZAivX"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}