{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_2-1_topic_identification_unsupervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKiMO3bqBgU"
      },
      "source": [
        "# **Topic Modeling**\n",
        "Topic identification is the challenge of automatically finding topics\n",
        "in a given text. This can be done in supervised and unsupervised ways. For example, an algorithm labels newspaper articles with known topics such\n",
        "as ”sports,” ”politics,” or ”culture.” In this case, we have predefined topics and labeled training data and could train our model in a supervised way. This is called topic classification. If we do not know the topics in advance and want our algorithm to find clusters of similar topics, we deal with topic modeling or topic discovery, in an unsupervised way [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "\n",
        "\n",
        "This notebook shows examples of unsupervised topic modeling with Gensim’s LDA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_rZCXXyJJe"
      },
      "source": [
        "## **Unsupervised topic modeling with Gensim’s LDA model**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a common technique used for unsupervised topic modeling. This method uses document embeddings, i.e., vector representations of documents. Then the vector’s dimensionality is reduced with techniques such as singular value decomposition (SVD). Unsupervised topic modeling techniques are often used as a preprocessing step for supervised topic identification [[1]](#scrollTo=1eUuDaNxZ_ms)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the ``nltk`` library and download ``wordnet``\n",
        "``nltk``(Natural Language Toolkit) is an open source Python library for natural language processing. For more details about the ``nltk`` library, please refer to [[2]](https://www.nltk.org/api/nltk.html#nltk.wsd.lesk).\n",
        "\n",
        "``wordnet`` is a lexical database of semantic relations between words in more than 200 languages. It links words into semantic relations [[3]](https://en.wikipedia.org/wiki/WordNet)."
      ],
      "metadata": {
        "id": "pDmDa7Fc-QvB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R5-7qiucf3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db40832-92e9-4b26-c1e3-388bfe5615f0"
      },
      "source": [
        "# Import the \"nltk\" module\n",
        "import nltk\n",
        "\n",
        "# Download the \"wordnet\" package by using the \"nltk\" module\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# The module '\"RegexpTokenizer\" is used to split a string into substrings using regular expressions\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# The module \"punkt\" is used to lemmatize the words using WordNet's built-in morphy function\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Download 'omw-1.4' to use Multilingual Wordnet Data from OMW with newer Wordnet versions (December 2021 release)\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# We import Python \"pprint\" library to print complex data structures in an easy to read format\n",
        "## For more details about the difference between print() and pprint() functions, please refer to [4]\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import ``gensim``\n",
        "``gensim`` is a Python library for topic modeling. It enables extraction of topics in an unsupervised way using LDA.\n",
        "For more details about Gensim's LDA model, please refer to [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)."
      ],
      "metadata": {
        "id": "myD112OO-i5Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ieidCq_yXdl"
      },
      "source": [
        "# Import gensim\n",
        "from gensim import corpora, models\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create documents"
      ],
      "metadata": {
        "id": "OkHyUtBD-4dv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXdV4e83TTGP"
      },
      "source": [
        "# Create a sample document list containing two documents\n",
        "doc_list = [\n",
        "           \"Black holes are dense points in space and they create deep gravity sinks. It is called black hole because beyond a certain region, not even light can escape the powerful tug of a black hole.\",\n",
        "           \"The Italian explorer Christopher Columbus officially set foot in the America, and claimed the land for Spain in October 12, 1492. Americans celebrate Columbus Day as a national holiday every year since 1937. This day is celebrated as Columbus Day in the United States, but the name varies on the international spectrum.\"\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize documents"
      ],
      "metadata": {
        "id": "zhKusjIe-9R-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEDkNc-TXLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46e6d27-e9ae-424f-b6b3-77ded0c0dd81"
      },
      "source": [
        "# Define a tokenizer to split each string into substrings\n",
        "## '\\w+' matches one or more alphanumeric characters\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Convert text to lowercase and tokenize\n",
        "for idx in range(len(doc_list)):\n",
        "    doc_list[idx] = doc_list[idx].lower()  \n",
        "    doc_list[idx] = tokenizer.tokenize(doc_list[idx])\n",
        "\n",
        "# Remove numbers, but not words that contain numbers\n",
        "doc_list = [[token for token in doc if not token.isnumeric()] for doc in doc_list]\n",
        "\n",
        "# Remove tokens that are only one character\n",
        "doc_list = [[token for token in doc if len(token) > 2] for doc in doc_list]\n",
        "\n",
        "# Print tokens\n",
        "pprint(doc_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['black',\n",
            "  'holes',\n",
            "  'are',\n",
            "  'dense',\n",
            "  'points',\n",
            "  'space',\n",
            "  'and',\n",
            "  'they',\n",
            "  'create',\n",
            "  'deep',\n",
            "  'gravity',\n",
            "  'sinks',\n",
            "  'called',\n",
            "  'black',\n",
            "  'hole',\n",
            "  'because',\n",
            "  'beyond',\n",
            "  'certain',\n",
            "  'region',\n",
            "  'not',\n",
            "  'even',\n",
            "  'light',\n",
            "  'can',\n",
            "  'escape',\n",
            "  'the',\n",
            "  'powerful',\n",
            "  'tug',\n",
            "  'black',\n",
            "  'hole'],\n",
            " ['the',\n",
            "  'italian',\n",
            "  'explorer',\n",
            "  'christopher',\n",
            "  'columbus',\n",
            "  'officially',\n",
            "  'set',\n",
            "  'foot',\n",
            "  'the',\n",
            "  'america',\n",
            "  'and',\n",
            "  'claimed',\n",
            "  'the',\n",
            "  'land',\n",
            "  'for',\n",
            "  'spain',\n",
            "  'october',\n",
            "  'americans',\n",
            "  'celebrate',\n",
            "  'columbus',\n",
            "  'day',\n",
            "  'national',\n",
            "  'holiday',\n",
            "  'every',\n",
            "  'year',\n",
            "  'since',\n",
            "  'this',\n",
            "  'day',\n",
            "  'celebrated',\n",
            "  'columbus',\n",
            "  'day',\n",
            "  'the',\n",
            "  'united',\n",
            "  'states',\n",
            "  'but',\n",
            "  'the',\n",
            "  'name',\n",
            "  'varies',\n",
            "  'the',\n",
            "  'international',\n",
            "  'spectrum']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dictionary\n",
        "We use ``Dictionary()`` method to create mapping between tokens and their IDs."
      ],
      "metadata": {
        "id": "LmxZoNof_QsH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUNr0cereINZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba28cf2-4510-4e53-edcb-95f65edd0c29"
      },
      "source": [
        "# Create a dictionary representation of the \"doc_list\"\"\n",
        "dictionary = corpora.Dictionary(doc_list)\n",
        "\n",
        "# Print keys and values of the \"dictionary\"\n",
        "for key, value in dictionary.items():\n",
        "  print(key, \" : \", value)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  :  and\n",
            "1  :  are\n",
            "2  :  because\n",
            "3  :  beyond\n",
            "4  :  black\n",
            "5  :  called\n",
            "6  :  can\n",
            "7  :  certain\n",
            "8  :  create\n",
            "9  :  deep\n",
            "10  :  dense\n",
            "11  :  escape\n",
            "12  :  even\n",
            "13  :  gravity\n",
            "14  :  hole\n",
            "15  :  holes\n",
            "16  :  light\n",
            "17  :  not\n",
            "18  :  points\n",
            "19  :  powerful\n",
            "20  :  region\n",
            "21  :  sinks\n",
            "22  :  space\n",
            "23  :  the\n",
            "24  :  they\n",
            "25  :  tug\n",
            "26  :  america\n",
            "27  :  americans\n",
            "28  :  but\n",
            "29  :  celebrate\n",
            "30  :  celebrated\n",
            "31  :  christopher\n",
            "32  :  claimed\n",
            "33  :  columbus\n",
            "34  :  day\n",
            "35  :  every\n",
            "36  :  explorer\n",
            "37  :  foot\n",
            "38  :  for\n",
            "39  :  holiday\n",
            "40  :  international\n",
            "41  :  italian\n",
            "42  :  land\n",
            "43  :  name\n",
            "44  :  national\n",
            "45  :  october\n",
            "46  :  officially\n",
            "47  :  set\n",
            "48  :  since\n",
            "49  :  spain\n",
            "50  :  spectrum\n",
            "51  :  states\n",
            "52  :  this\n",
            "53  :  united\n",
            "54  :  varies\n",
            "55  :  year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create bag-of-words representation\n",
        "We use ``doc2bow()`` method to convert ``dictionary`` into the bag-of-words format. This method computes the frequency of each word and transforms documents to a vectorized form. "
      ],
      "metadata": {
        "id": "qvOgNxZR_ZfX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWdnHOgeqOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64aaf125-799d-453d-b097-9f1db0b0fd43"
      },
      "source": [
        "# Create bag-of-words representation of the documents \n",
        "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
        "\n",
        "# Print \"corpus\"\n",
        "pprint(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1),\n",
            "  (1, 1),\n",
            "  (2, 1),\n",
            "  (3, 1),\n",
            "  (4, 3),\n",
            "  (5, 1),\n",
            "  (6, 1),\n",
            "  (7, 1),\n",
            "  (8, 1),\n",
            "  (9, 1),\n",
            "  (10, 1),\n",
            "  (11, 1),\n",
            "  (12, 1),\n",
            "  (13, 1),\n",
            "  (14, 2),\n",
            "  (15, 1),\n",
            "  (16, 1),\n",
            "  (17, 1),\n",
            "  (18, 1),\n",
            "  (19, 1),\n",
            "  (20, 1),\n",
            "  (21, 1),\n",
            "  (22, 1),\n",
            "  (23, 1),\n",
            "  (24, 1),\n",
            "  (25, 1)],\n",
            " [(0, 1),\n",
            "  (23, 6),\n",
            "  (26, 1),\n",
            "  (27, 1),\n",
            "  (28, 1),\n",
            "  (29, 1),\n",
            "  (30, 1),\n",
            "  (31, 1),\n",
            "  (32, 1),\n",
            "  (33, 3),\n",
            "  (34, 3),\n",
            "  (35, 1),\n",
            "  (36, 1),\n",
            "  (37, 1),\n",
            "  (38, 1),\n",
            "  (39, 1),\n",
            "  (40, 1),\n",
            "  (41, 1),\n",
            "  (42, 1),\n",
            "  (43, 1),\n",
            "  (44, 1),\n",
            "  (45, 1),\n",
            "  (46, 1),\n",
            "  (47, 1),\n",
            "  (48, 1),\n",
            "  (49, 1),\n",
            "  (50, 1),\n",
            "  (51, 1),\n",
            "  (52, 1),\n",
            "  (53, 1),\n",
            "  (54, 1),\n",
            "  (55, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set training parameters and create model\n",
        "In this section, we set the following training parameters:\n",
        "* ``num_topics``: It presents the number of topics (number of dimensions) and can be freely chosen. In this example, we set ``num_topics = 2``. It means we ask our unsupervised clustering algorithm to group our dataset into 2 topics. [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "* ``chunksize``: It controls how many documents are processed at a time in the training algorithm. Increasing the ``chunksize`` speeds up the training process. In this example, we set ``chunksize = 10``, which is more than the number of documents, so we process all data at a single time. ``chunksize`` can influence the quality of the model [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html).\n",
        "* ``passes``: It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. In this example, we set ``passes = 10``.\n",
        "* ``iterations``: It defines how often we repeat a particular loop over each document [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html). In this example, we set ``iterations = 400``.\n",
        "* ``alpha``: It is a hyperparameter that controls the prior probability distribution over topic weights in each document. In this example, we set  ``alpha='auto'``.\n",
        "* ``eta``: It is a hyperparameter that controls the prior probability distribution over word weights in each topic. In this example, we set  ``eta='auto'``.\n"
      ],
      "metadata": {
        "id": "8rwQnNHnAToP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcmRi7AseqIX"
      },
      "source": [
        "# Set training parameters\n",
        "num_topics = 2\n",
        "chunksize = 10\n",
        "passes = 10\n",
        "iterations = 400\n",
        "alpha='auto'\n",
        "eta='auto'\n",
        "\n",
        "# Create a dictionary \"id2word\"\n",
        "temp = dictionary[0]  # This is only to load the dictionary\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "# Create topic model\n",
        "model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha=alpha, \n",
        "    eta=eta,\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the top 10 word tokens\n",
        "In this section, we use ``show_topics()`` method to list the most 10 probable tokens of each topic. The tokens are listed in descending order of their topic-specific probabilities  [[6]](https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf). Gensim's LDA model uses token frequency data of each document to calculate the topic-specific propability. The sum of all topic-specific probabilities is 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "F-WI4lYeBXXw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI1olErdfjXV",
        "outputId": "14804482-8e3a-4c9b-c094-53e0ee74f1cf"
      },
      "source": [
        "# Use \"show_topics()\" method to list topic-specific probability of each token\n",
        "## \"num_words\" is used to define the number of tokens to be listed for each topic\n",
        "## Set \"formattted=True\" to list tokens and their topic-specific probability scores as string\n",
        "## Set \"formatted=False\" to list as tuple\n",
        "top_10_word_tokens = model.show_topics(num_words=10, formatted=False)\n",
        "\n",
        "# Print the top 10 word tokens with their topic-specific probabilities\n",
        "pprint(top_10_word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  [('the', 0.059344474),\n",
            "   ('black', 0.05311735),\n",
            "   ('hole', 0.03787877),\n",
            "   ('and', 0.03203366),\n",
            "   ('powerful', 0.021743223),\n",
            "   ('tug', 0.021655953),\n",
            "   ('can', 0.021655044),\n",
            "   ('create', 0.021637557),\n",
            "   ('space', 0.021615326),\n",
            "   ('called', 0.021614939)]),\n",
            " (1,\n",
            "  [('the', 0.09992744),\n",
            "   ('day', 0.046697255),\n",
            "   ('columbus', 0.046594404),\n",
            "   ('and', 0.028399862),\n",
            "   ('foot', 0.019078067),\n",
            "   ('explorer', 0.019069921),\n",
            "   ('national', 0.019069491),\n",
            "   ('united', 0.019044688),\n",
            "   ('officially', 0.019032152),\n",
            "   ('states', 0.019023782)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print a topic representation\n",
        "In this section, we choose 3 tokens which have the highest topic-specific probabilities and print them as topic representations of each topic."
      ],
      "metadata": {
        "id": "ecHc6e9ATcMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 3 word tokens representing each topic\n",
        "for index, topic in model.show_topics(formatted=False, num_words= 3):\n",
        "    print('Topic: {} \\nWords: {}'.format(index, ' '.join([w[0] for w in topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx7-RhAapkSU",
        "outputId": "eeed44e0-a4dc-40f4-cad4-f0b962db4e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: the black hole\n",
            "Topic: 1 \n",
            "Words: the day columbus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://www.nltk.org/api/nltk.html#nltk.wsd.lesk\n",
        "- [3] https://en.wikipedia.org/wiki/WordNet\n",
        "- [4] https://docs.python.org/3/library/pprint.html\n",
        "- [5] https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
        "- [6] https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf"
      ],
      "metadata": {
        "id": "1eUuDaNxZ_ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ],
      "metadata": {
        "id": "kF9klNmjaDob"
      }
    }
  ]
}