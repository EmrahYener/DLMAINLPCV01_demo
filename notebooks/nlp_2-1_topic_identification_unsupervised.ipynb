{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_2-1_topic_identification_unsupervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKiMO3bqBgU"
      },
      "source": [
        "# **Topic Identification**\n",
        "Topic identification is the challenge of automatically finding topics\n",
        "in a given text. This can be done in supervised and unsupervised ways. For example, an algorithm labels newspaper articles with known topics such\n",
        "as ”sports,” ”politics,” or ”culture.” In this case, we have predefined topics and labeled training data and could train our model in a supervised way. This is called topic classification. If we do not know the topics in advance and want our algorithm to find clusters of similar topics, we deal with topic modeling or topic discovery, in an unsupervised way [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "\n",
        "\n",
        "This notebook shows examples of unsupervised topic identification with Gensim’s LDA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_rZCXXyJJe"
      },
      "source": [
        "## **Unsupervised topic modeling with Gensim’s LDA model**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a common technique used for unsupervised topic modeling. This method uses document embeddings, i.e., vector representations of documents. Then the vector’s dimensionality is reduced with techniques such as singular value decomposition (SVD). Unsupervised topic modeling techniques are often used as a preprocessing step for supervised topic identification [[1]](#scrollTo=1eUuDaNxZ_ms)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the ``nltk`` library and download ``wordnet``\n",
        "``nltk``(Natural Language Toolkit) is an open source Python library for natural language processing. For more details about the ``nltk`` library, please refer to [[2]](https://www.nltk.org/api/nltk.html#nltk.wsd.lesk).\n",
        "\n",
        "``wordnet`` is a lexical database of semantic relations between words in more than 200 languages. It links words into semantic relations [[3]](https://en.wikipedia.org/wiki/WordNet)."
      ],
      "metadata": {
        "id": "pDmDa7Fc-QvB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R5-7qiucf3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf85e33-599d-4977-9a18-12f0db2b96a2"
      },
      "source": [
        "# Import the nltk module\n",
        "import nltk\n",
        "\n",
        "# Download the \"wordnet\" package by using the nltk module\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# The module 'RegexpTokenizer' is used to split a string into substrings using regular expressions\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# The module \"punkt\" is used to lemmatize the words using WordNet's built-in morphy function\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Download 'omw-1.4' to use Multilingual Wordnet Data from OMW with newer Wordnet versions (December 2021 release)\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import ``gensim``\n",
        "``gensim`` is a Python library for topic modeling. It enables extraction of topics in an unsupervised way using LDA.\n",
        "For more details about Gensim's LDA model, please refer to [[4]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)."
      ],
      "metadata": {
        "id": "myD112OO-i5Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ieidCq_yXdl"
      },
      "source": [
        "# Import gensim\n",
        "from gensim import corpora, models\n",
        "import gensim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create documents"
      ],
      "metadata": {
        "id": "OkHyUtBD-4dv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXdV4e83TTGP"
      },
      "source": [
        "# Create a sample document list containing two documents\n",
        "doc_list = [\n",
        "           \"Black holes are dense points in space and they create deep gravity sinks. It is called black hole because beyond a certain region, not even light can escape the powerful tug of a black hole.\",\n",
        "           \"The Italian explorer Christopher Columbus officially set foot in the America, and claimed the land for Spain in October 12, 1492. Americans celebrate Columbus Day as a national holiday every year since 1937. This day is celebrated as Columbus Day in the United States, but the name varies on the international spectrum.\"\n",
        "           ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize documents"
      ],
      "metadata": {
        "id": "zhKusjIe-9R-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEDkNc-TXLv"
      },
      "source": [
        "# Define a tokenizer to split each string into substrings\n",
        "## '\\w+' matches one or more alphanumeric characters\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Convert text to lowercase and tokenize\n",
        "for idx in range(len(doc_list)):\n",
        "    doc_list[idx] = doc_list[idx].lower()  \n",
        "    doc_list[idx] = tokenizer.tokenize(doc_list[idx])\n",
        "\n",
        "# Remove numbers, but not words that contain numbers\n",
        "doc_list = [[token for token in doc if not token.isnumeric()] for doc in doc_list]\n",
        "\n",
        "# Remove words that are only one character\n",
        "doc_list = [[token for token in doc if len(token) > 2] for doc in doc_list]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dictionary"
      ],
      "metadata": {
        "id": "LmxZoNof_QsH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUNr0cereINZ"
      },
      "source": [
        "# Create a dictionary representation of the document list\n",
        "dictionary = corpora.Dictionary(doc_list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize documents and create corpus\n",
        "We compute the frequency of each word and transform documents to a vectorized form.  "
      ],
      "metadata": {
        "id": "qvOgNxZR_ZfX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWdnHOgeqOV"
      },
      "source": [
        "# Bag-of-words representation of the documents\n",
        "corpus = [dictionary.doc2bow(doc) for doc in doc_list]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBKYacZreqKr",
        "outputId": "470a848c-e367-48f3-9ba2-7f1fb1080c30"
      },
      "source": [
        "# Print the number of unique tokens and the number of documents\n",
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 56\n",
            "Number of documents: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set training parameters and create model\n",
        "In this section, we will set the following training parameters:\n",
        "* ``num_topics``: It presents the number of topics (number of dimensions) and can be freely chosen. For example, if we set this parameter to 10,\n",
        "we ask our unsupervised clustering algorithm to group our dataset into 10 topics represented as a 10-dimensional vector for each of our documents. In this example, we set ``num_topics = 2`` [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "* ``chunksize``: It controls how many documents are processed at a time in the training algorithm. Increasing the ``chunksize`` speeds up the training process, at least as long as the chunk of documents easily fit into memory. In this example, we set ``chunksize = 10``, which is more than the number of documents, so we process all the data at a single time. ``chunksize`` can influence the quality of the model [[4]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html).\n",
        "* ``passes``: It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. In this example, we set ``passes = 10``.\n",
        "* ``iterations``: It defines how often we repeat a particular loop over each document [[4]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html). In this example, we set ``iterations = 400``.\n",
        "* ``alpha``: It is a parameter that controls the prior distribution over topic weights in each document. In this example, we set  ``alpha='auto'``.\n",
        "* ``eta``: It is a parameter for the prior distribution over word weights in each topic. In this example, we set  ``eta='auto'``.\n"
      ],
      "metadata": {
        "id": "8rwQnNHnAToP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcmRi7AseqIX"
      },
      "source": [
        "# Set training parameters.\n",
        "num_topics = 2\n",
        "chunksize = 2\n",
        "passes = 10\n",
        "iterations = 400\n",
        "alpha='auto'\n",
        "eta='auto'\n",
        "\n",
        "# Make an \"index to word\" dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "# Create topic model\n",
        "model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha=alpha, \n",
        "    eta=eta,\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the top 10 topic words\n",
        "In this section, we use ``show_topics()`` method to show a list of the most 10 probable words for each topic. The words are listed in descending order of their topic-specific probabilities  [[5]](https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf). \n",
        "\n",
        "To calculate the topic-specific propability, LDA model uses word distributions for each topic and looks at the words that appear frequently within the topic."
      ],
      "metadata": {
        "id": "F-WI4lYeBXXw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI1olErdfjXV",
        "outputId": "e305d251-29b7-4434-9b29-cea732616d13"
      },
      "source": [
        "# Use \"show_topics()\" method to list probability score of each token\"\n",
        "## \"num_words\" is used to define the number of words to be listed for each topic\n",
        "## \"formattted_True\" is used to show topic probability scores as string\n",
        "## \"formatted=False\" is used to show topic probability scores as tuple\n",
        "top_10_topic_words = model.show_topics(num_words=10, formatted=False)\n",
        "\n",
        "# Print topic scores for all tokens\n",
        "from pprint import pprint\n",
        "pprint(top_10_topic_words)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  [('the', 0.0592251),\n",
            "   ('black', 0.053588435),\n",
            "   ('hole', 0.0377618),\n",
            "   ('and', 0.032134674),\n",
            "   ('can', 0.02175805),\n",
            "   ('gravity', 0.021742798),\n",
            "   ('even', 0.021730296),\n",
            "   ('escape', 0.021726195),\n",
            "   ('powerful', 0.021709928),\n",
            "   ('create', 0.021709377)]),\n",
            " (1,\n",
            "  [('the', 0.10010287),\n",
            "   ('columbus', 0.0466471),\n",
            "   ('day', 0.046641182),\n",
            "   ('and', 0.028461445),\n",
            "   ('explorer', 0.019063286),\n",
            "   ('varies', 0.019056935),\n",
            "   ('italian', 0.019042756),\n",
            "   ('set', 0.019030467),\n",
            "   ('land', 0.019028725),\n",
            "   ('since', 0.019027684)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print 3-word topic representations\n",
        "In this section, we print the top 3 words which represent each topic."
      ],
      "metadata": {
        "id": "ecHc6e9ATcMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 3 word tokens representing each topic\n",
        "for index, topic in model.show_topics(formatted=False, num_words= 3):\n",
        "    print('Topic: {} \\nWords: {}'.format(index, ' '.join([w[0] for w in topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx7-RhAapkSU",
        "outputId": "f279c609-8b5c-40a9-93da-03fac14a2165"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: the black hole\n",
            "Topic: 1 \n",
            "Words: the columbus day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://www.nltk.org/api/nltk.html#nltk.wsd.lesk\n",
        "- [3] https://en.wikipedia.org/wiki/WordNet\n",
        "- [4] https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
        "- [5] https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf"
      ],
      "metadata": {
        "id": "1eUuDaNxZ_ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ],
      "metadata": {
        "id": "kF9klNmjaDob"
      }
    }
  ]
}