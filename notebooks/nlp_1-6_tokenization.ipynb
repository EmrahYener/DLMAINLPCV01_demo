{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-6_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_X42HQ1pF84"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we demonstrate how to implement tokenization.\n",
        "In many cases, it is the first step in NLP tasks.\n",
        "\n",
        "Tokenization is the process of splitting up textual input data, e.g., a sentence or a paragraph into meaningful units [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "Word tokenization frequently uses word pieces such as word stems, prefixes,\n",
        "and suffixes (e.g., “-ation”). \n",
        "Tokenization is used to transform the input data of an NLP model into a more meaningful space [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "This notebook shows examples for the following tokenization:\n",
        "- Word Tokenization\n",
        "- Sentence Tokenization"
      ],
      "metadata": {
        "id": "p07VlvadPlrX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPahqp-pF9i"
      },
      "source": [
        "## **Word-Tokenization**\n",
        "\n",
        "In this section we will use spaCy for word tokenization.\n",
        "\n",
        "SpaCy is one of the most famous framework for NLP. It can be used for the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "More information about spaCy please refer to  [[2]](#scrollTo=op-j6UywUt5i)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For word tokenization, we will follow the following steps:\n",
        "1. Import spaCy library\n",
        "2. Load the language model (English)\n",
        "3. Create a spaCy document\n",
        "4. Access the word tokens by iterating over the document object\n",
        "5. Print the word tokens"
      ],
      "metadata": {
        "id": "A8bwMX4XTpmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import spaCy library"
      ],
      "metadata": {
        "id": "7mLLi589N-M2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFaXafBN12i"
      },
      "source": [
        "# Import spaCy library to process the text\n",
        "## spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "## spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. \n",
        "## It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning based on [3].\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load language model"
      ],
      "metadata": {
        "id": "dPvXKO7tOJQR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVtW6JEpF9f"
      },
      "source": [
        "# Import \"en_core_web_sm\" English language model by using spaCy library\n",
        "## It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities based on [4].\n",
        "## It is optimized for CPU and its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer based on [5].\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create spaCy document and tokenize text"
      ],
      "metadata": {
        "id": "hXK00FUnQb97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we create a text, spaCy automatically tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
        "\n",
        "You san see the processing pipeline in the following picture based on [[6]](#scrollTo=op-j6UywUt5i):\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "EQQJE8atougw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n_iU6SVpF9k"
      },
      "source": [
        "# Create a sample document\n",
        "## During the document creation process, spaCy will automatically tokenize the given text.\n",
        "## Then it will save the tokenized text in the doc file.\n",
        "doc = sp(u'I am non-vegetarian, send me the menu at abs-xyz@gmail.com. \"They are going to U.K. and the to the U.S.A\"')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print word tokens"
      ],
      "metadata": {
        "id": "yrKPnbDRQ3gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each token in the doc file:\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMAW7r2qb4_",
        "outputId": "91188a7e-83af-4370-9c9b-6426ba7c266b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "send\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abs-xyz@gmail.com\n",
            ".\n",
            "\"\n",
            "They\n",
            "are\n",
            "going\n",
            "to\n",
            "U.K.\n",
            "and\n",
            "the\n",
            "to\n",
            "the\n",
            "U.S.A\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0iRL_ZpF9o"
      },
      "source": [
        "## Sentence-Tokenization\n",
        "If we want to split up a given text into individual sentences, then we use sentence tokenization [[1]](#scrollTo=op-j6UywUt5i).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already created a doc file by using spaCy. We will use the same doc file for sentence tokenization.\n",
        "\n",
        "As explained, spaCy automatically performs the tokenization process during document creation. For sentence tokenization, we will simply use the \"doc.sents\" attribute of the \"Sentencizer\" class in the spaCy. For more detail please refer to [[7]](#scrollTo=op-j6UywUt5i)."
      ],
      "metadata": {
        "id": "XxTtiboWP3vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print tokenized sentences"
      ],
      "metadata": {
        "id": "aC6BLU8ZSQlo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYFYuprlpF9q",
        "outputId": "853cdb17-10f0-473e-dc4f-7755b7314e89"
      },
      "source": [
        "# We will use the \"doc.sents\" attribute for sentence tokenization.\n",
        "## It iterates over sentences in the doc.\n",
        "## Then defines the first token of each sentence.To decide whether a token starts a sentence, spaCy assigns a boolean value to each token.\n",
        "## This will be either True or False for all tokens. For more detail please refer to [7]\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am non-vegetarian, send me the menu at abs-xyz@gmail.com.\n",
            "\"They are going to U.K. and the to the U.S.A\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [1] NLP and Computer Vision_DLMAINLPCV01 Course Book\n",
        "- [2] https://spacy.io/\n",
        "- [3] https://spacy.io/usage/spacy-101\n",
        "- [4] https://spacy.io/models\n",
        "- [5] https://spacy.io/models/en\n",
        "- [6] https://spacy.io/usage/processing-pipelines\n",
        "- [7] https://spacy.io/api/sentencizer"
      ],
      "metadata": {
        "id": "op-j6UywUt5i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDAQDmXpF91"
      },
      "source": [
        "Copyright © 2021 IU International University of Applied Sciences"
      ]
    }
  ]
}