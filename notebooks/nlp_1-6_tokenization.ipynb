{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-6_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_X42HQ1pF84"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1- Introduction**\n",
        "In this section, we demonstrate how to implement some NLP tasks using spaCy.\n",
        "In many cases, tokenization is the first step in NLP tasks.\n",
        "\n",
        "\n",
        "SpaCy is one of the most famous framework for NLP. It can be used for the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others.\n",
        "\n",
        "More information about spaCy please refer to  [[1]](#scrollTo=op-j6UywUt5i)\n",
        "\n",
        "### **Content**\n",
        "In this notebook some basic examples for following topics are shown:\n",
        "- Word Tokenization\n",
        "- Sentence Tokenization"
      ],
      "metadata": {
        "id": "p07VlvadPlrX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPahqp-pF9i"
      },
      "source": [
        "## **1- Word-Tokenization**\n",
        "\n",
        "For word tokenization, we will follow the following steps:\n",
        "* Import the spaCy library\n",
        "* Load the language model (English)\n",
        "* Create a spaCy document\n",
        "* Access the word tokens by iterating over the document object\n",
        "* Print the word tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVtW6JEpF9f"
      },
      "source": [
        "# Import spaCy library to process the text\n",
        "## spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "## spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. \n",
        "## It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning based on [2].\n",
        "import spacy\n",
        "\n",
        "# Import \"en_core_web_sm\" by using spaCy library\n",
        "## It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities based on [3].\n",
        "## It is optimized for CPU and its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer based on [4].\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n_iU6SVpF9k",
        "outputId": "9d469516-c297-497e-a0d5-791f885f249a"
      },
      "source": [
        "# Create document\n",
        "doc = sp(u'I am non-vegetarian, send me the menu at abs-xyz@gmail.com. \"They are going to U.K. and the to the U.S.A\"')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "send\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abs-xyz@gmail.com\n",
            ".\n",
            "\"\n",
            "They\n",
            "are\n",
            "going\n",
            "to\n",
            "U.K.\n",
            "and\n",
            "the\n",
            "to\n",
            "the\n",
            "U.S.A\n",
            "\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0iRL_ZpF9o"
      },
      "source": [
        "## **2- Sentence-Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYFYuprlpF9q",
        "outputId": "3eee08e1-0948-405c-b7b9-d6e2ab5cb00e"
      },
      "source": [
        "# Print the whole sentences from the document 'doc'\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am non-vegetarian, send me the menu at abs-xyz@gmail.com.\n",
            "\"They are going to U.K. and the to the U.S.A\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3- References**\n",
        "\n",
        "- [1] https://spacy.io/\n",
        "- [2] https://spacy.io/usage/spacy-101\n",
        "- [3] https://spacy.io/models\n",
        "- [4] https://spacy.io/models/en"
      ],
      "metadata": {
        "id": "op-j6UywUt5i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDAQDmXpF91"
      },
      "source": [
        "Copyright © 2021 IU International University of Applied Sciences"
      ]
    }
  ]
}