{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-6_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_X42HQ1pF84"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1- Introduction**\n",
        "In this section, we demonstrate how to implement some NLP tasks using spaCy.\n",
        "In many cases, tokenization is the first step in NLP tasks.\n",
        "\n",
        "\n",
        "SpaCy is one of the most famous framework for NLP. It can be used for the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others.\n",
        "\n",
        "More information about spaCy please refer to  [[1]](#scrollTo=op-j6UywUt5i)\n",
        "\n",
        "### **Content**\n",
        "In this notebook some basic examples for following topics are shown:\n",
        "- Word Tokenization\n",
        "- Sentence Tokenization"
      ],
      "metadata": {
        "id": "p07VlvadPlrX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPahqp-pF9i"
      },
      "source": [
        "## **2- Word-Tokenization**\n",
        "\n",
        "For word tokenization, we will follow the following steps:\n",
        "* Import the spaCy library\n",
        "* Load the language model (English)\n",
        "* Create a spaCy document\n",
        "* Access the word tokens by iterating over the document object\n",
        "* Print the word tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVtW6JEpF9f"
      },
      "source": [
        "# Import spaCy library to process the text\n",
        "## spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "## spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. \n",
        "## It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning based on [2].\n",
        "import spacy\n",
        "\n",
        "# Import \"en_core_web_sm\" English language model by using spaCy library\n",
        "## It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities based on [3].\n",
        "## It is optimized for CPU and its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer based on [4].\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a sample document.\n",
        "\n",
        "When you create a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
        "\n",
        "You san see the processing pipeline in the following picture based on [5]:\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "EQQJE8atougw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n_iU6SVpF9k"
      },
      "source": [
        "# Create a sample document\n",
        "## During document creation process, spaCy will automatically tokenize the given text.\n",
        "## Then it will save the tokanized text in the doc file.\n",
        "doc = sp(u'I am non-vegetarian, send me the menu at abs-xyz@gmail.com. \"They are going to U.K. and the to the U.S.A\"')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will now print each token in the doc file:\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "GWMAW7r2qb4_",
        "outputId": "e13c3a51-ac6d-4029-f79f-a8b73b99d1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "send\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abs-xyz@gmail.com\n",
            ".\n",
            "\"\n",
            "They\n",
            "are\n",
            "going\n",
            "to\n",
            "U.K.\n",
            "and\n",
            "the\n",
            "to\n",
            "the\n",
            "U.S.A\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0iRL_ZpF9o"
      },
      "source": [
        "## **3- Sentence-Tokenization**\n",
        "If we want to split up a given text into individual sentences, the we use sentence tokenization.\n",
        "\n",
        "In the code examples above, we have created a doc file by using spaCy. \n",
        "\n",
        "For sentence tokenization, we will use \"doc.sents\" attribute of the \"Sentencizer\" class in the spaCy.\n",
        "\n",
        "Sentencizer class is a pipeline component for rule-based sentence boundary detection based on [6].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYFYuprlpF9q",
        "outputId": "919b2fb7-f101-447a-a332-29989ca5ac4b"
      },
      "source": [
        "# We will use doc.sents attribute for sentence tokenization.\n",
        "## It iterates over sentences in the doc.\n",
        "## To decide whether a token starts a sentence, spaCy assigns a boolean value to each token.\n",
        "## This will be either True or False for all tokens. For more detail please refer to [6]\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am non-vegetarian, send me the menu at abs-xyz@gmail.com.\n",
            "\"They are going to U.K. and the to the U.S.A\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4- References**\n",
        "\n",
        "- [1] https://spacy.io/\n",
        "- [2] https://spacy.io/usage/spacy-101\n",
        "- [3] https://spacy.io/models\n",
        "- [4] https://spacy.io/models/en\n",
        "- [5] https://spacy.io/usage/processing-pipelines\n",
        "- [6] https://spacy.io/api/sentencizer"
      ],
      "metadata": {
        "id": "op-j6UywUt5i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDAQDmXpF91"
      },
      "source": [
        "Copyright © 2021 IU International University of Applied Sciences"
      ]
    }
  ]
}