{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-6_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_X42HQ1pF84"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting up textual input data, e.g., a sentence or a paragraph into meaningful units [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "Word tokenization frequently uses word pieces such as word stems, prefixes,\n",
        "and suffixes (e.g., “-ation”). \n",
        "Tokenization is used to transform the input data of an NLP model into a more meaningful space [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "This notebook shows examples for the following tokenization:\n",
        "- Word Tokenization\n",
        "- Sentence Tokenization"
      ],
      "metadata": {
        "id": "p07VlvadPlrX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPahqp-pF9i"
      },
      "source": [
        "## **Word tokenization**\n",
        "\n",
        "In this section we will use spaCy for word tokenization.\n",
        "\n",
        "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning based on [3]. For example, it supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=op-j6UywUt5i). More information about spaCy please refer to  [[2]](#scrollTo=op-j6UywUt5i)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For word tokenization, we will apply the following steps:\n",
        "1. Import spaCy library\n",
        "2. Load the language model (English)\n",
        "3. Create a spaCy document\n",
        "4. Access the word tokens by iterating over the document\n",
        "5. Print the word tokens"
      ],
      "metadata": {
        "id": "A8bwMX4XTpmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import spaCy library"
      ],
      "metadata": {
        "id": "7mLLi589N-M2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFaXafBN12i"
      },
      "source": [
        "# Import spaCy library to process the text\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load language model"
      ],
      "metadata": {
        "id": "dPvXKO7tOJQR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVtW6JEpF9f"
      },
      "source": [
        "# Import \"en_core_web_sm\" English language model by using spaCy library\n",
        "## It is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities based on [4].\n",
        "## It is optimized for CPU and its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer based on [5].\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create spaCy document and tokenize text"
      ],
      "metadata": {
        "id": "hXK00FUnQb97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we create a text, spaCy automatically tokenizes the text to produce a Doc object. The following figure demonstrates the processing pipeline of a given text to produce a Doc object based on [[6]](#scrollTo=op-j6UywUt5i):\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "EQQJE8atougw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n_iU6SVpF9k"
      },
      "source": [
        "# Create a sample spaCy document\n",
        "## During the document creation process, spaCy automatically tokenizes the given text.\n",
        "## Then it saves the tokenized text in the document.\n",
        "doc = sp(u'I am non-vegetarian, send me the menu at abs-xyz@gmail.com. \"They are going to U.K. and then to the U.S.A\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print word tokens"
      ],
      "metadata": {
        "id": "yrKPnbDRQ3gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each token in the document:\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMAW7r2qb4_",
        "outputId": "6ead26b2-534d-416c-ff79-146c6f6d75dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "send\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abs-xyz@gmail.com\n",
            ".\n",
            "\"\n",
            "They\n",
            "are\n",
            "going\n",
            "to\n",
            "U.K.\n",
            "and\n",
            "then\n",
            "to\n",
            "the\n",
            "U.S.A\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0iRL_ZpF9o"
      },
      "source": [
        "## Sentence tokenization\n",
        "If we want to split up a given text into individual sentences, then we use sentence tokenization [[1]](#scrollTo=op-j6UywUt5i). We have already created a document by using spaCy. We will use the same document for sentence tokenization.\n",
        "\n",
        "As explained, spaCy automatically performs the tokenization process during document creation. For sentence tokenization, we will simply use the \"doc.sents\" attribute of the \"Sentencizer\" class in spaCy. For more details about the \"Sentencizer\" class, please refer to [[7]](#scrollTo=op-j6UywUt5i)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print tokenized sentences"
      ],
      "metadata": {
        "id": "aC6BLU8ZSQlo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYFYuprlpF9q",
        "outputId": "12d8d8fc-fc26-4644-8afd-a667da833f9e"
      },
      "source": [
        "# Use the \"doc.sents\" attribute for sentence tokenization.\n",
        "## It iterates over sentences in the document.\n",
        "## Then it defines the first word token of each sentence.To decide whether a token starts a sentence, spaCy assigns a boolean value to each token.\n",
        "## This will be either True or False for all tokens.\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am non-vegetarian, send me the menu at abs-xyz@gmail.com.\n",
            "\"They are going to U.K. and then to the U.S.A\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [1] NLP and Computer Vision_DLMAINLPCV01 Course Book\n",
        "- [2] https://spacy.io/\n",
        "- [3] https://spacy.io/usage/spacy-101\n",
        "- [4] https://spacy.io/models\n",
        "- [5] https://spacy.io/models/en\n",
        "- [6] https://spacy.io/usage/processing-pipelines\n",
        "- [7] https://spacy.io/api/sentencizer"
      ],
      "metadata": {
        "id": "op-j6UywUt5i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDAQDmXpF91"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}